{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Ch3-Lecture 9\n",
    "\n",
    "## Information Encoding\n",
    "\n",
    "The strategy of how to represent information as a quantum state provides the context of how to design the quantum algorithm and what speedups one can hope to harvest. The actual procedure of encoding data into the quantum system is part of the algorithm and may account for a crucial part of the complexity.1 Theoretical frameworks, software and hardware that address the interface between the classical memory and the quantum device are therefore central for technological implementations of quantum machine learning. Issues of efficiency, precision and noise play an important role in performance evaluation. This is even more true since most quantum machine learning algorithms deliver probabilistic results and the entire routine—including state preparation—may have to be repeated many times. These arguments call for a thorough discussion of “data encoding” approaches and routines, which is why we dedicate this entire chapter to questions of data representation with quantum states.\n",
    "\n",
    "\n",
    "Table 1\n",
    ": Comparison of the four encoding strategies for a dataset of $M$ inputs with $N$ features each. While basis, amplitude and Hamiltonian encoding aim at representing a full data set by\n",
    "the quantum system, qsample encoding works a little different in that it represents a probability distribution over random variables. It therefore does not have a dependency on the number of inputs $M$ . *Only certain datasets or models can be encoded in this time.\n",
    "\n",
    "| Encoding   \t | Number of qubits   \t | Runtime of state prep   \t                  | Input features   \t |\n",
    "|--------------|----------------------|--------------------------------------------|--------------------|\n",
    "| Basis\t       | $N$ \t                | $\\mathcal{O}(MN)$\t                         | \tBinary            |\n",
    "| Amplitude\t   | $\\log N$\t            | $\\mathcal{O}(MN)/\\mathcal{O}(\\log(MN))^*$\t | \tContinuous        |\n",
    "| Qsample\t     | $N$\t                 | $\\mathcal{O}(2^N)/\\mathcal{O}(N)^*$\t       | Binary\t            |\n",
    "| Hamiltonian\t | $\\log N$\t                    | $\\mathcal{O}(MN)/\\mathcal{O}(\\log(MN))^*$\t | Continuous\t        |\n",
    "\n",
    "\n",
    "A central figure of merit for state preparation is the asymptotic runtime, and an overview of runtimes for the four encoding methods is provided in the above Table. In machine learning, the input of the algorithm is the data, and an efficient algorithm is efficient in the dimension of the data inputs N and the number of data points M . In quantum computing an efficient algorithm has a polynomial runtime with respect to the number of qubits. Since data can be encoded into qubits or amplitudes, the expression “efficient” can have different meanings in quantum machine learning, and this easily gets confusing. To facilitate the discussion, we will call an algorithm either amplitude-efficient or qubit-efficient, depending on what we consider as an input. It is obvious that if the data is encoded into the amplitudes or operators of a quantum system (as in amplitude and Hamiltonian encoding), amplitude-efficient state preparation routines are also efficient in terms of the data set. If we encode data into qubits, qubit-efficient state preparation is efficient in the data set size. We will see that there are some very interesting cases in which we encode data into amplitudes or Hamiltonians, but can guarantee qubit-efficient state preparation routines. In these cases, we prepare data in time which is logarithmic in the data size itself. Of course, this requires either a very specific access to or a very special structure of the data. Before we start, a safe assumption when nothing more about the hardware is known is that we have a n-qubit system in the ground state |0...0\u0002 and that the data is accessible from a classical memory. In some cases we will also require some specific classical preprocessing. We consider data sets D = {x1 , ..., xM } of N -dimensional real feature vectors. Note that many algorithms require the labels to be encoded in qubits entangled with the inputs, but for the sake of simplicity we will focus on unlabelled data in this chapter.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basis Encoding\n",
    "\n",
    "Assume we are given a binary dataset D where each pattern xm ∈ D is a binary\n",
    "m\n",
    "m\n",
    "string of the form xm = (bm\n",
    "1 , ..., bN ) with bi ∈ {0, 1} for i = 1, ..., N . We can prepare\n",
    "m\n",
    "a superposition of basis states |x \u0002 that qubit-wise correspond to the binary input\n",
    "patterns,\n",
    "M\n",
    "1 \u0002 m\n",
    "|D\u0002 = √\n",
    "|x \u0002.\n",
    "(5.1)\n",
    "M m=1\n",
    "For example, given two binary inputs x1 = (01, 01)T , x2 = (11, 10)T , where fea-\n",
    "tures are encoded with a binary precision of τ = 2, we can write them as binary pat-\n",
    "terns x1 = (0110), x2 = (1110). These patterns can be associated with basis states\n",
    "|0110\u0002, |1110\u0002, and the full data superposition reads\n",
    "1\n",
    "1\n",
    "|D\u0002 = √ |0101\u0002 + √ |1110\u0002.\n",
    "2\n",
    "2\n",
    "(5.2)\n",
    "The amplitude vector corresponding to State (5.1) has entries √1M for basis states\n",
    "that are associated with a binary pattern from the dataset, and zero entries otherwise.\n",
    "For Eq. (5.2), the amplitude vector is given by\n",
    "1\n",
    "1\n",
    "α = (0, 0, 0, 0, 0, √ , 0, 0, 0, 0, 0, 0, 0, 0, √ , 0)T\n",
    "2\n",
    "2\n",
    "\n",
    "Since—except in very low dimensions—the total number of amplitudes 2N τ is much\n",
    "larger than the number of nonzero amplitudes M , basis encoded datasets generally\n",
    "give rise to sparse amplitude vectors.\n",
    "\n",
    "#### Preparing Superpositions of Inputs\n",
    "\n",
    "n elegant way to construct such ‘data superpositions’ in time linear in M and N\n",
    "has been introduced by Ventura, Martinez and others [3, 4], and will be summarised\n",
    "here as an example of basis encoded state preparation. The circuit for one step in the\n",
    "routine is shown in Fig. 5.2. We will simplify things by considering binary inputs in\n",
    "which every bit represents one feature, or τ = 1.\n",
    "We require a quantum system\n",
    "|l1 , ..., lN ; a1 , a2 ; s1 , ..., sN \u0002\n",
    "with three registers: a loading register of N qubits |l1 , ..., lN \u0002, the ancilla register\n",
    "|a1 , a2 \u0002 with two qubits and the N -qubit storage register |s1 , ..., sN \u0002. We start in the\n",
    "ground state and apply a Hadamard to the second ancilla to get\n",
    "1\n",
    "1\n",
    "√ |0, ..., 0; 0, 0; 0, ..., 0\u0002 + √ |0, ..., 0; 0, 1; 0, ..., 0\u0002.\n",
    "2\n",
    "2\n",
    "The left term, flagged with a2 = 0, is called the memory branch, while the right term,\n",
    "flagged with a2 = 1, is the processing branch. The algorithm iteratively loads patterns\n",
    "into the loading register and ‘breaks away’ the right size of terms from the processing\n",
    "\n",
    "branch to add it to the memory branch (Fig. 5.3). This way the superposition of\n",
    "patterns is ‘grown’ step by step.\n",
    "To explain how one iteration works, assume that the first m training vectors have\n",
    "already been encoded after iterations 1, ..., m of the algorithm. This leads to the\n",
    "state\n",
    "\u0003\n",
    "m\n",
    "M −m\n",
    "1 \u0002\n",
    "(m)\n",
    "k\n",
    "k\n",
    "|0, ..., 0; 01; 0, ..., 0\u0002.\n",
    "|0, ..., 0; 00; x1 , ..., xN \u0002 +\n",
    "|ψ \u0002 = √\n",
    "M\n",
    "M\n",
    "k=1\n",
    "(5.3)\n",
    "The memory branch stores the first m inputs in its storage register, while the storage\n",
    "register of the processing branch is in the ground state. In both branches the loading\n",
    "register is also in the ground state.\n",
    "To execute the (m + 1)th step of the algorithm, write the (m + 1)th pattern xm+1 =\n",
    "m+1\n",
    "(x1 , ..., xNm+1 ) into the qubits of the loading register (which will write it into both\n",
    "branches). This can be done by applying an X gate to all qubits that correspond\n",
    "to nonzero bits in the input pattern. Next, in the processing branch the pattern gets\n",
    "copied into the storage register using a CNOT gate on each of the N qubits. To limit\n",
    "the execution to the processing branch only we have to control the CNOTs with the\n",
    "second ancilla being in a2 = 1. This leads to\n",
    "1 \u0002 m+1\n",
    "|x1 , ..., xNm+1 ;00; x1k , ..., xNk \u0002\n",
    "√\n",
    "M k=1\n",
    "\u0003\n",
    "M − m m+1\n",
    "|x1 , ..., xNm+1 ; 01; x1m+1 , ..., xNm+1 \u0002.\n",
    "+\n",
    "M\n",
    "m\n",
    "Using a CNOT gate, we flip a1 = 1 if a2 = 1, which is only true for the processing\n",
    "branch. Afterwards apply the single qubit unitary\n",
    "⎛\u0006\n",
    "Ua2 (μ) = ⎝\n",
    "μ−1\n",
    "μ\n",
    "−1\n",
    "√\n",
    "μ\n",
    "⎞\n",
    "√1\n",
    "\u0006 μ ⎠\n",
    "μ−1\n",
    "μ\n",
    "with μ = M + 1 − (m + 1) to qubit a2 but controlled by a1 . On the full quantum\n",
    "state, this operation amounts to\n",
    "1loading ⊗ ca1 Ua2 (μ) ⊗ 1storage .\n",
    "This splits the processing branch into two subbranches, one that can be “added” to\n",
    "the memory branch and one that will remain the processing branch for the next step,\n",
    "\n",
    "To add the subbranch marked by |a1 a2 \u0002 = |10\u0002 to the memory branch, we have to\n",
    "flip a1 back to 1. To confine this operation to the desired subbranch we can condition\n",
    "it on an operation that compares if the loading and storage register are in the same\n",
    "state (which is only true for the two processing subbranches), and that a2 = 1 (which\n",
    "is only true for the desired subbranch). Also, the storage register of the processing\n",
    "branch as well as the loading register of both branches has to be reset to the ground\n",
    "state by reversing the previous operations, before the next iteration begins. After the\n",
    "(m + 1)th iteration we start with a state similar to Eq. (5.3) but with m → m + 1.\n",
    "The routine requires O(MN ) steps, and succeeds with certainty.\n",
    "There are interesting alternative proposals for architectures of quantum Random\n",
    "Access Memories, devices that load patterns ‘in parallel’ into a quantum register.\n",
    "These devices are designed to query an index register |m\u0002 and load the mth binary\n",
    "pattern into a second register in basis encoding, |m\u0002|0...0\u0002 → |m\u0002|xm \u0002. Most impor-\n",
    "tantly, this operation can be executed in parallel. Given a superposition of the index\n",
    "register, the quantum random access memory is supposed to implement the operation\n",
    "M −1\n",
    "M −1\n",
    "1 \u0002\n",
    "1 \u0002\n",
    "|m\u0002|0...0\u0002 → √\n",
    "|m\u0002|xm \u0002.\n",
    "√\n",
    "M m=0\n",
    "M m=0\n",
    "(5.4)\n",
    "We will get back to this in the next section, where another step allows us to prepare\n",
    "amplitude encoded quantum states. Ideas for architectures which realise this kind of5.1 Basis Encoding\n",
    "145\n",
    "‘query access’ in logarithmic time regarding the number of items to be loaded have\n",
    "been brought forward [5], but a hardware realising such an operation is still an open\n",
    "challenge [6, 7].\n",
    "\n",
    "#### Computing in Basis Encoding\n",
    "\n",
    "Acting on binary features encoded as qubits gives us the most computational freedom\n",
    "to design quantum algorithms. In principle, each operation on bits that we can execute\n",
    "on a classical computer can be done on a quantum computer as well. The argument\n",
    "is roughly the following: A Toffoli gate implements the logic operation\n",
    "Input\n",
    "000\n",
    "001\n",
    "010\n",
    "011\n",
    "100\n",
    "101\n",
    "110\n",
    "111\n",
    "Output\n",
    "000\n",
    "001\n",
    "010\n",
    "011\n",
    "100\n",
    "101\n",
    "111\n",
    "110\n",
    "and is a universal gate for Boolean logic [8]. Universality implies that any binary\n",
    "function f : {0, 1}⊗N → {0, 1}⊗D can be implemented by a succession of Toffoli\n",
    "gates and possibly some ‘garbage’ bits. The special role of the Toffoli gate stems\n",
    "from the fact that it is reversible. If one only sees the state after the operation, one\n",
    "can deduce the exact state before the operation (i.e., if the first two bits are 11,\n",
    "flip the third one). No information is lost in the operation, which is in physical\n",
    "terms a non-dissipative operation. In mathematical terms the matrix representing\n",
    "the operation has an inverse. Reversible gates, and hence also the Toffoli gate, can\n",
    "be implemented on a quantum computer. In conclusion, if any classical algorithm\n",
    "can efficiently be formulated in terms of Toffoli gates, and these can always be\n",
    "implemented on a quantum computer, this means that any classical algorithm can\n",
    "efficiently be translated to a quantum algorithm. The reformulation of a classical\n",
    "algorithm with Toffoli gates may however have a large polynomial overhead and\n",
    "slow down the routines significantly.\n",
    "Note that once encoded into a superposition, the data inputs can be processed in\n",
    "quantum parallel (see Sect. 3.2.4). For example, if a routine\n",
    "A(|x\u0002 ⊗ |0\u0002) → |x\u0002 ⊗ |f (x)\u0002146\n",
    "5 Information Encoding\n",
    "is known to implement a machine learning model f and write the binary predic-\n",
    "tion f (x) into the state of a qubit, we can perform inference in parallel on the data\n",
    "superposition,\n",
    "M\n",
    "M\n",
    "1 \u0002 m\n",
    "1 \u0002 m\n",
    "A √\n",
    "|x \u0002 ⊗ |0\u0002 → √\n",
    "|x \u0002 ⊗ |f (xm )\u0002.\n",
    "M m=1\n",
    "M m=1\n",
    "From this superposition one can extract statistical information, for example by mea-\n",
    "suring the last qubit. Such a measurement reveals the expectation value of the pre-\n",
    "diction of the entire dataset.\n",
    "\n",
    "#### Sampling from a Qubit\n",
    "\n",
    "As the previous example shows, the result of a quantum machine learning model can\n",
    "be basis encoded as well. For binary classification this only requires one qubit. If the\n",
    "qubit is in state |f (x)\u0002 = |1\u0002 the prediction is 1 and if the qubit is in state |f (x)\u0002 = |0\u0002\n",
    "the prediction is 0. A superposition can be interpreted as a probabilistic output that\n",
    "provides information on the uncertainty of the result.\n",
    "In order to read out the state of the qubit we have to measure it, and we want to\n",
    "briefly address how to obtain the prediction estimate from measurements, as well\n",
    "as what number of measurements are needed for a reliable prediction. The field of\n",
    "reconstructing a quantum state from measurements is called quantum tomography,\n",
    "and there are very sophisticated ways in which samples from these measurements\n",
    "can be used to estimate the density matrix that describe the state. Here we consider a\n",
    "much simpler problem, namely to estimate the probability of measuring basis state\n",
    "|0\u0002 or |1\u0002. In other words, we are just interested in the diagonal elements of the\n",
    "density matrix, not in the entire state. Estimating the ouptut of a quantum model in\n",
    "basis encoding is therefore a ‘classical’ rather than a ‘quantum task’.\n",
    "Let the final state of the output qubit be given by the density matrix\n",
    "ρ=\n",
    "ρ00 ρ01\n",
    ".\n",
    "ρ10 ρ11\n",
    "We need the density matrix formalism from Chap. 3 here because the quantum com-\n",
    "puter may be in a state where other qubits are entangled with the output qubit, and the\n",
    "single-qubit-state is therefore a mixed state. We assume that the quantum algorithm\n",
    "is error-free, so that repeating it always leads to precisely the same density matrix ρ\n",
    "to take measurements from. The diagonal elements ρ00 and ρ11 fulfil ρ00 + ρ11 = 1\n",
    "and give us the probability of measuring the qubit in state |0\u0002 or |1\u0002 respectively. We\n",
    "associate ρ11 with the probabilistic output f (x) = p which gives us the probability\n",
    "that model f predicts y = 1 for the input x.5.1 Basis Encoding\n",
    "147\n",
    "To get an estimate p̂ of p we have to repeat the entire algorithm S times and\n",
    "perform a computational basis measurement on the output qubit in each run. This\n",
    "produces a set of samples Ω = {y1 , ..., yS } of outcomes, and we assume the samples\n",
    "stem from a distribution that returns 0 with probability 1 − p and 1 with probability\n",
    "p. Measuring a single qubit is therefore equivalent to sampling from a Bernoulli\n",
    "distribution, a problem widely investigated in statistics.2 There are various strategies\n",
    "of how to get an estimate p̂ from samples Ω. Most prominent, maximum likelihood\n",
    "estimation leads to the rather intuitive ‘frequentist estimator’ p̂ = p̄ = S1 Si=1 yi\n",
    "which is nothing else than the average over all outcomes.\n",
    "An important question is how many samples from the single qubit measurement\n",
    "we need to estimate p with error \u0006. In physics language, we want an ‘error bar’ \u0006 of our\n",
    "estimation p̂ ± \u0006, or the confidence interval [p̂ − \u0006, p̂ + \u0006]. A confidence interval is\n",
    "valid for a pre-defined confidence level, for example of 99%. The confidence level has\n",
    "the following meaning: If we have different sets of samples S1 , ..., SS and compute\n",
    "estimators and confidence intervals for each of them, p̂S1 ± \u0006S1 , ..., p̂SS ± \u0006SS , the\n",
    "confidence level is the proportion of sample sets for which the true value p lies within\n",
    "the confidence interval around p̂. The confidence level is usually expressed by a so\n",
    "called z-value, for example, a z-value of 2.58 corresponds to a confidence of 99%.\n",
    "This correspondence can be looked up in tables.\n",
    "Frequently used is the Wald interval which is suited for cases of large S and\n",
    "p ≈ 0.5. The error \u0006 can be calculated as\n",
    "\u0003\n",
    "\u0006=z\n",
    "p̂(1 − p̂)\n",
    ".\n",
    "S\n",
    "This is maximised for p = 0.5, so that we can assume the overall error of our esti-\n",
    "mation \u0006 to be at most\n",
    "z\n",
    "\u0006≤ √\n",
    "2 S\n",
    "with a confidence level of z. In other words, for a given \u0006 and z we need O(\u0006−2 )\n",
    "samples of the qubit measurement. If we want to have an error bar of at most \u0006 = 0.1\n",
    "and a confidence level of 99% we need about 167 samples, and an error of \u0006 = 0.01\n",
    "with confidence 99% requires at most 17, 000 samples. This is a vast number, but\n",
    "only needed if the estimator is equal to 0.5, which is the worst case scenario of an\n",
    "undecided classifier (and we may not want to rely on the decision very much in any\n",
    "case). One can also see that the bound fails for p → 0, 1 [9].\n",
    "There are other estimates that also work when p is close to either zero or one. A\n",
    "more refined alternative is the Wilson score interval [10] with the following estimator\n",
    "for p,\n",
    "z2\n",
    "1\n",
    "p̄\n",
    "+\n",
    ",\n",
    "p̂ =\n",
    "2\n",
    "2S\n",
    "1 + zS\n",
    "2 Bernoulli sampling is equivalent to a (biased) coin toss experiment: We flip a coin S times and\n",
    "want to estimate the bias p, i.e. with what probability the coin produces ‘heads’.148\n",
    "5 Information Encoding\n",
    "Fig. 5.4 Relationship\n",
    "between the sample size S\n",
    "and the mean value\n",
    "p̄ = S1 Si=1 yi for different\n",
    "errors \u0006 for the Wilson score\n",
    "interval of a Bernoulli\n",
    "parameter estimation\n",
    "problem as described in the\n",
    "text\n",
    "and the error\n",
    "\u0006=\n",
    "z\n",
    "2\n",
    "1 + zS\n",
    "p̄(1 − p̄)\n",
    "z2\n",
    "+ 2\n",
    "S\n",
    "4S\n",
    "1\n",
    "2\n",
    ",\n",
    "with p̄ being the average of all samples as defined above. Again this is maximised\n",
    "for p̄ = 0.5 and with a confidence level z we can state that the overall error of our\n",
    "estimation is bounded by\n",
    "\u0003\n",
    "S + z2\n",
    "\u0006 ≤ z2\n",
    ".\n",
    "4S 2\n",
    "This can be solved for S as\n",
    "S≤\n",
    "\u00062\n",
    "\u0006\n",
    "z 4 (16\u00062 +1)\n",
    "+ z2\n",
    "\u00064\n",
    ".\n",
    "8\u00062\n",
    "With the Wilson score, a confidence level of 99% suggests that we need 173 single\n",
    "qubit measurements to guarantee an error of less than 0.1. However, now we can\n",
    "test the cases p̄ = 0, 1 for which S = z 2 ( 2\u00061 − 1). For \u0006 = 0.1 we only need about 27\n",
    "measurements for the same confidence level (see Fig. 5.4)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
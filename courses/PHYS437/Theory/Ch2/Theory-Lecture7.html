<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Ch2-Lecture 7 &#8212; Practical Quantum Computing for Scientists 2022.02.24 alpha documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/bootstrap-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Lecture 1 online material" href="../Ch1/Ch1-online.html" />
    <link rel="prev" title="Ch2-Lecture 6" href="Theory-Lecture6.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../../../_static/js/jquery-1.12.4.min.js"></script>
<script type="text/javascript" src="../../../../_static/js/jquery-fix.js"></script>
<script type="text/javascript" src="../../../../_static/bootstrap-3.4.1/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../../../../_static/bootstrap-sphinx.js"></script>

  </head><body>

  <div id="navbar" class="navbar navbar-inverse navbar-default ">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../index.html">
          Practical QC for Scientists</a>
        <span class="navbar-text navbar-version pull-left"><b>2022.02.24</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../../index.html">437</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../../index.html">Contents <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../index.html">Courses</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../index.html">PHYS 437</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../archives/archives.html">Archives</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../help/index.html">HOWTOs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../help/IBM_quantum.html">Using IBM quantum Cloud</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <section class="tex2jax_ignore mathjax_ignore" id="ch2-lecture-7">
<h1>Ch2-Lecture 7<a class="headerlink" href="#ch2-lecture-7" title="Permalink to this headline">¶</a></h1>
<section id="entropy-and-entanglement-distillation">
<h2>Entropy and Entanglement Distillation<a class="headerlink" href="#entropy-and-entanglement-distillation" title="Permalink to this headline">¶</a></h2>
<p>The theme of the last two lectures has been of a quantum information theoretic nature — we have studied cloning (or rather, lack thereof), entanglement, and non-local correlations. Before progressing to our next main theme of quantum algorithms, we now give a brief taste of more advanced ideas in quantum information. In the process, we will continue getting used to working with quantum states in both the state vector and density operator formalisms.</p>
<p>The main questions we ask in this lecture are the following:</p>
<ul class="simple">
<li><p>How can we quantify the “amount” of entanglement in a composite quantum system?</p></li>
<li><p>Under what conditions can “less entangled” states be “converted” to “more entangled” states?</p></li>
</ul>
<p>The first question will require the foundational concept of entropy, introduced in the context of classical information theory by Claude Shannon in 1948. The entropy is worthy of a lecture in itself, being an extremely important quantity. The second question above is tied to the discovery of entanglement distillation, in which, similar to the age-old process of distilling potable water from salt water (or more fittingly for our analogy, “pure” water from “dirty” water), one can “distill” pure entanglement from noisy entanglement.</p>
</section>
<section id="entropy">
<h2>Entropy<a class="headerlink" href="#entropy" title="Permalink to this headline">¶</a></h2>
<p>One of the most influential scientific contributions of the 20th century was the 1948 paper of Claude Shannon, “A Mathematical Theory of Communication”, which single-handed founded the field of information theory. Roughly, the aim of information theory is to study information transmission and compression. For this, Shannon introduced the notion of entropy, which intuitively quantifies “how random” a data source is, or the “average information content” of the source. It turns out that a quantum generalization of entropy will be vital to quantifying entanglement; as such, we begin by defining and motivating the classical Shannon entropy.</p>
<section id="shannon-entropy">
<h3>Shannon entropy<a class="headerlink" href="#shannon-entropy" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a discrete random variable taking values from set <span class="math notranslate nohighlight">\( \left\{x_1,\dots,x_n\right\} \)</span>, where <span class="math notranslate nohighlight">\( \text{Pr}\left(x_i\right) :=  \text{Pr}\left(X=x_i\right)\)</span> denotes the probability that <span class="math notranslate nohighlight">\(X\)</span> takes value <span class="math notranslate nohighlight">\(x_i\)</span> . Then, the Shannon entropy <span class="math notranslate nohighlight">\(H(X)\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
H(X)= \sum_{i=1}^{n} -\text{Pr}\left(x_i\right) \log \left( \text{Pr}\left(x_i\right) \right)
\]</div>
<p>Here, the logarithm is taken base 2, and we define <span class="math notranslate nohighlight">\(0.\log\left(0\right)=0 \)</span>.</p>
<p>Before getting our hands dirty understanding <span class="math notranslate nohighlight">\(H(x)\)</span>, let us step back and motivate it via data compression. Suppose we have a data source whose output we wish to transmit from (say) Germany to Canada. Naturally, we may wish to first <em>compress</em> the data, so that we need to transmit as few bits as possible between the two countries. Furthermore, a compression scheme is useless unless we are later able to <em>recover</em> or <em>uncompress</em> the data in Canada. This raises the natural question: <strong>How few bits can one transmit, so as to ensure recovery of the data on the receiving end?</strong> Remarkably, Shannon’s noiseless coding theorem says that this quantity is given by the entropy! Roughly, the theorem says that in order to reliably transmit N i.i.d. (independently and identically distributed) random variables <span class="math notranslate nohighlight">\(X_i\)</span> from a random source <span class="math notranslate nohighlight">\(X\)</span>, it is necessary and sufficient to instead send <span class="math notranslate nohighlight">\(H(X)\)</span> bits of communication.</p>
<p>We now explore the sense in which <span class="math notranslate nohighlight">\(H(X)\)</span> indeed quantifies the “randomness” or “uncertainty” of <span class="math notranslate nohighlight">\(X\)</span> by considering two boundary cases. In the first boundary case, <span class="math notranslate nohighlight">\(X\)</span> models a fair coin flip, i.e. <span class="math notranslate nohighlight">\(X\)</span> takes value HEADS or TAILS with probability 1/2 each. Then,</p>
<div class="math notranslate nohighlight">
\[
H(X)=- \frac{1}{2}  \log\left( \frac{1}{2} \right)- \frac{1}{2}  \log\left( \frac{1}{2} \right)= \frac{1}{2} + \frac{1}{2} =1
\]</div>
<p>Therefore, we interpret a fair coin as encoding, on average, <em>one</em> bit of information. Alternatively, in the information transmission setting, we would need to transmit a single bit to convey the answer of the coin flip from Germany to Canada. This intuitively makes sense — since the outcome of the coin flip is completely random, there is no way to guess its outcome in Canada with success probability greater than 1/2 (i.e. a random guess).</p>
<section id="send-it-after-class-1">
<h4>Send it after class 1<a class="headerlink" href="#send-it-after-class-1" title="Permalink to this headline">¶</a></h4>
<p>Suppose random variable <span class="math notranslate nohighlight">\(Y\)</span> models a biased coin, e.g. takes value HEADS and TAILS with probability 1 and 0, respectively. What is <span class="math notranslate nohighlight">\(H(Y)\)</span>?</p>
<p>In the above exercise, there is no “uncertainty”; we know the outcome will be HEADS. Thus, in the communication setting, one can interpret this as saying zero bits of communication are required to transmit the outcome of the coin flip from Germany to Canada (assuming both Germany and Canada know the probabilities of obtaining HEADS and TAILS beforehand). Indeed, the answer to the exercise above is <span class="math notranslate nohighlight">\(H(Y) = 0\)</span>.</p>
</section>
<section id="send-it-after-class-2">
<h4>Send it after class 2<a class="headerlink" href="#send-it-after-class-2" title="Permalink to this headline">¶</a></h4>
<p>Let random variable <span class="math notranslate nohighlight">\(Z\)</span> take values in set <span class="math notranslate nohighlight">\(\{0, 1, 2\}\)</span> with probabilities <span class="math notranslate nohighlight">\(\{1/4, 1/4, 1/2\}\)</span>, respectively. What is <span class="math notranslate nohighlight">\(H(Z)\)</span></p>
<p>The entropy formula is odd-looking; to understand how it is derived, the key observation is the intuition behind the coin flip examples, which says that “when an event is <em>less</em> likely to happen, it reveals <em>more</em> information”. To capture this intuition, Shannon started with a formula for <em>information content</em> <span class="math notranslate nohighlight">\(I(x_i)\)</span>, which for any possible event <span class="math notranslate nohighlight">\(x_i\)</span> for random variable <span class="math notranslate nohighlight">\(X\)</span>, is given by</p>
<div class="math notranslate nohighlight">
\[
I  \left(x_i\right)= \log\left( \frac{1}{ \text{Pr}\left(x_i\right) } \right)=- \log\left( \text{Pr}\left(x_i\right) \right)
\]</div>
<p>Since the log function is strictly monotonically increasing (i.e. <span class="math notranslate nohighlight">\(I(x) &gt; I(y)\)</span> if <span class="math notranslate nohighlight">\(x &gt; y\)</span> for <span class="math notranslate nohighlight">\(x, y  \in  (0, \infty)\)</span>), it holds that <span class="math notranslate nohighlight">\(I(x_i)\)</span> captures the idea that “rare events yield more information”. But <span class="math notranslate nohighlight">\(I(x)\)</span> also has three other important properties we expect of an “information measure”; here are the first two:</p>
<ol class="simple">
<li><p>(Information is non-negative) <span class="math notranslate nohighlight">\(I(x)  \geq  0\)</span></p></li>
<li><p>if <span class="math notranslate nohighlight">\( \text{Pr}\left(x\right)  = 1\)</span>, then <span class="math notranslate nohighlight">\(I(x) = 0\)</span>. (If an event occurs with certainty, said occurrence conveys no information)</p></li>
</ol>
<p>For the third important property, we ask — why did we use the log function? Why not any other monotonically increasing function satisfying properties (1) and (2) above? Recall that, by definition, two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent if</p>
<div class="math notranslate nohighlight">
\[
 \text{Pr}\left(X = x \text{ and } Y = y\right)=  \text{Pr}\left(X = x\right)\text{Pr}\left(Y = y\right) .
\]</div>
<p>Moreover, if <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, then intuitively one expects the information conveyed by the joint event <span class="math notranslate nohighlight">\(z := (X = x \text{ and } Y = y)\)</span> to be <em>additive</em>, i.e. <span class="math notranslate nohighlight">\(I(z) = I(x) + I(y)\)</span>. But this is precisely what the information content I(x) satisfies, due to its use of the log function.</p>
</section>
</section>
<section id="send-it-after-class-3">
<h3>Send it after class 3<a class="headerlink" href="#send-it-after-class-3" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be independent random variables. Then, for <span class="math notranslate nohighlight">\(z := (X = x \text{ and } Y =y)\)</span>, express <span class="math notranslate nohighlight">\(I(z)\)</span> in terms of <span class="math notranslate nohighlight">\(I(x)\)</span> and <span class="math notranslate nohighlight">\(I(y)\)</span>.</p>
<p>We can now use the information content to derive the formula for entropy — <span class="math notranslate nohighlight">\(H(X)\)</span> is simply the <em>expected</em> information content over all possible events <span class="math notranslate nohighlight">\( \left\{  x_1 \dots  x_n \right\} \)</span>. (Recall here that for random variable <span class="math notranslate nohighlight">\(X\)</span> taking values <span class="math notranslate nohighlight">\(x  \in  \{x_i\}\)</span>, its expectation <span class="math notranslate nohighlight">\(E[x]\)</span> is given by <span class="math notranslate nohighlight">\(E[x] =  \sum_{i} \text{Pr}\left(x_i\right). xi\)</span> ) This is precisely why at the start of this section, we referred to <span class="math notranslate nohighlight">\(H(x)\)</span> as the <strong>average</strong> information content of a data source.</p>
</section>
<section id="von-neumann-entropy">
<h3>Von Neumann Entropy<a class="headerlink" href="#von-neumann-entropy" title="Permalink to this headline">¶</a></h3>
<p>Recall the first aim of this lecture was to use entropy to measure entanglement. For this, we shall require a quantum generalization of the Shannon entropy <span class="math notranslate nohighlight">\(H(X)\)</span>, named as  the <em>von Neumann entropy</em> <span class="math notranslate nohighlight">\(S( \rho )\)</span>, for density operator <span class="math notranslate nohighlight">\( \rho \)</span>. To motivate this definition, let us recall the “hierarchy of matrix classes” we introduced in discussing measurements:</p>
<ul class="simple">
<li><p>Hermitian operators, <span class="math notranslate nohighlight">\(\text{Herm}  \left(\mathbb{C}^d\right) \)</span> , which generalize the real numbers.</p></li>
<li><p>Positive semidefinite operators, <span class="math notranslate nohighlight">\(\text{Pos} \left(\mathbb{C}^d\right)\)</span> , which generalize the non-negative real numbers.</p></li>
<li><p>Orthogonal projection operators, <span class="math notranslate nohighlight">\(\Pi\left(\mathbb{C}^d\right)\)</span>, which generalize the set <span class="math notranslate nohighlight">\(\{0, 1\}\)</span>.</p></li>
</ul>
<p>Note that <span class="math notranslate nohighlight">\(\Pi\left(\mathbb{C}^d\right) \subseteq \text{Pos} \left(\mathbb{C}^d\right) \subseteq \text{Herm}  \left(\mathbb{C}^d\right)\)</span> , and that the notion of “generalize” above means the eigenvalues of the operators fall into the respective class the operators generalize. (For example, matrices in <span class="math notranslate nohighlight">\(\text{Pos}  \left(\mathbb{C}^d\right)\)</span> have non-negative eigenvalues.) Applying this same interpretation to the set of <em>density operators</em> acting on <span class="math notranslate nohighlight">\(\mathbb{C}^d\)</span>, <span class="math notranslate nohighlight">\(D  \left(\mathbb{C}^d\right)\)</span> , we thus have that density operators generalize the notion of a <em>probability distribution</em>. Indeed, any probability distribution can be embedded into a diagonal density matrix.</p>
<section id="send-it-after-class-4">
<h4>Send it after class 4<a class="headerlink" href="#send-it-after-class-4" title="Permalink to this headline">¶</a></h4>
<p>Let <span class="math notranslate nohighlight">\( \left\{p_i\right\}^d_{i=1} \)</span> denote a probability distribution. Define diagonal matrix <span class="math notranslate nohighlight">\( \rho  \in  \mathcal{L}  \left( \mathbb{C}^d\right) \)</span> such that <span class="math notranslate nohighlight">\( \rho  \left(i,i\right)=p_i \)</span> . Show that <span class="math notranslate nohighlight">\( \rho \)</span> is a density matrix.</p>
<p>Since the eigenvalues <span class="math notranslate nohighlight">\( \lambda_i \left( \rho \right) \)</span> of a density operator <span class="math notranslate nohighlight">\( \rho  \in D \left( \mathbb{C} ^d\right) \)</span> form a probability distribution, the natural approach for defining a quantum entropy formula is to apply the classical Shannon entropy to the spectrum of <span class="math notranslate nohighlight">\( \rho \)</span>:</p>
<div class="math notranslate nohighlight">
\[
S \left( \rho \right):=H \left( \left\{ \lambda _i \left( \rho \right) \right\}_{i=1}^d \right)= \sum_{i=1}^{d}- \lambda_i \left(\rho\right) \log\left(\lambda_i \left(\rho\right)\right)
\]</div>
<p><strong>Operator functions</strong>. It is important to pause now and take stock of what we have done in defining <span class="math notranslate nohighlight">\(S( \rho )\)</span> : In order to apply a function <span class="math notranslate nohighlight">\(f :  \mathbb{R}  \mapsto  \mathbb{R} \)</span> to a Hermitian matrix <span class="math notranslate nohighlight">\(H  \in  \text{Herm}  \left(\mathbb{C}^d\right)\)</span> , we instead applied <span class="math notranslate nohighlight">\(f\)</span> to the <em>eigenvalues</em> of <span class="math notranslate nohighlight">\(H\)</span>. Why does this “work”? Let us look at the Taylor series expansion of <span class="math notranslate nohighlight">\(f\)</span> , which for e.g. <span class="math notranslate nohighlight">\(f = e^x\)</span> is (the series converges for all x)</p>
<div class="math notranslate nohighlight">
\[
e^x= \sum_{i=1}^{\infty} \frac{x^n}{n!}=1+x+ \frac{x^2}{2!}+\frac{x^3}{3!}+\dots
\]</div>
<p>This suggests an idea — to define <span class="math notranslate nohighlight">\(e^H\)</span> , perhaps we can substitute <span class="math notranslate nohighlight">\(H\)</span> in the right hand side of the Taylor series expansion of <span class="math notranslate nohighlight">\(e^x\)</span> :</p>
<div class="math notranslate nohighlight">
\[
e^H:= I+H+ \frac{H^2}{2!}+\frac{H^3}{3!}+\dots
\]</div>
<p>Indeed, this leads to our desired definition; that to generalize the function <span class="math notranslate nohighlight">\(f(x) = e^x\)</span> to Hermitian matrices, we apply <span class="math notranslate nohighlight">\(f\)</span> to the eigenvalues of <span class="math notranslate nohighlight">\(H\)</span>.</p>
</section>
<section id="send-it-after-class-5">
<h4>Send it after class 5<a class="headerlink" href="#send-it-after-class-5" title="Permalink to this headline">¶</a></h4>
<p>Let <span class="math notranslate nohighlight">\(H\)</span> have spectral decomposition <span class="math notranslate nohighlight">\(H =  \sum_{i=1} \lambda_i \left| \lambda_i \right\rangle\!\left\langle \lambda_i \right|\)</span>. Show that</p>
<div class="math notranslate nohighlight">
\[
e^H=\sum_{i=1} e^{\lambda_i} \left| \lambda_i \right\rangle\!\left\langle \lambda_i \right|
\]</div>
<p>This idea of applying functions <span class="math notranslate nohighlight">\(f :  \mathbb{R}  \mapsto  \mathbb{R} \)</span> to the eigenvalues of Hermitian operators is used so frequently in quantum information that we give such “generalized <span class="math notranslate nohighlight">\(f\)</span> ” a name — <em>operator functions</em>. In the case of <span class="math notranslate nohighlight">\(S( \rho )\)</span>, by setting <span class="math notranslate nohighlight">\(f(x) =  \log\left(x\right)\)</span>, we can rewrite <span class="math notranslate nohighlight">\(S( \rho )\)</span> as</p>
<div class="math notranslate nohighlight">
\[
S( \rho )=- \text{Tr}\left( \rho  \log\left( \rho \right) \right)
\]</div>
</section>
<section id="send-it-after-class-6">
<h4>Send it after class 6<a class="headerlink" href="#send-it-after-class-6" title="Permalink to this headline">¶</a></h4>
<ol class="simple">
<li><p>Verify <span class="math notranslate nohighlight">\(\sum_{i=1}^{d}- \lambda_i \left(\rho\right) \log\left(\lambda_i \left(\rho\right)\right)=- \text{Tr}\left( \rho  \log\left( \rho \right) \right)\)</span></p></li>
<li><p>Let <span class="math notranslate nohighlight">\(f (x) = x^2\)</span> . What is <span class="math notranslate nohighlight">\(f(X)\)</span>, <span class="math notranslate nohighlight">\(X\)</span> being the Pauli <span class="math notranslate nohighlight">\(X\)</span> operator? Why does this yield the same results as multiplying <span class="math notranslate nohighlight">\(X\)</span> by itself via matrix multiplication?</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(f (x) =\sqrt{x}\)</span>. For any pure state <span class="math notranslate nohighlight">\( \left|\psi\right\rangle  \in  \mathbb{C} ^d \)</span> define rank one density operator <span class="math notranslate nohighlight">\( \rho = \left| \psi \right\rangle\!\left\langle \psi \right| \)</span>. What is <span class="math notranslate nohighlight">\(\sqrt{\rho}\)</span>?</p></li>
<li><p>What is <span class="math notranslate nohighlight">\(\sqrt{Z}\)</span> for the Pauli <span class="math notranslate nohighlight">\(Z\)</span> operator? Is it uniquely defined?</p></li>
</ol>
</section>
<section id="properties-of-the-von-neumann-entropy">
<h4>Properties of the von Neumann entropy<a class="headerlink" href="#properties-of-the-von-neumann-entropy" title="Permalink to this headline">¶</a></h4>
<p>Let us now see how properties of <span class="math notranslate nohighlight">\(H(X)\)</span> carry over to <span class="math notranslate nohighlight">\(S( \rho )\)</span>. These will prove crucial in our understanding of quantifying entanglement shortly.</p>
<ol class="simple">
<li><p><em>When does a quantum state have no entropy?</em>
Recall in our biased coin flip example that if an outcome occurs with probability 1 in our distribution, then <span class="math notranslate nohighlight">\(H(X) = 0\)</span>. Quantumly, the analogue of this statement is that <span class="math notranslate nohighlight">\(S( \rho ) = 0\)</span> if and only if <span class="math notranslate nohighlight">\( \rho \)</span> is a pure state, i.e. <span class="math notranslate nohighlight">\( \rho = \left| \psi \right\rangle\!\left\langle \psi \right| \)</span> for some <span class="math notranslate nohighlight">\( \left|\psi\right\rangle  \in  \mathbb{C}^d\)</span> . This is because a recall a pure state is the special case of a mixed state in which one of the states in the preparation procedure is picked with certainty.</p></li>
<li><p><em>When does a quantum state have maximum entropy?</em>
We saw that when <span class="math notranslate nohighlight">\(X\)</span> represents a fair coin flip, <span class="math notranslate nohighlight">\(H(X) = 1\)</span>. This is, in fact, the unique distribution maximizing <span class="math notranslate nohighlight">\(H\)</span>. Applying this directly to the definition of <span class="math notranslate nohighlight">\(S( \rho )\)</span>, we find that <span class="math notranslate nohighlight">\(S( \rho )\)</span> is maximized over all <span class="math notranslate nohighlight">\( \rho  \in D \left( \mathbb{C}^2\right)\)</span> if and only if both eigenvalues of <span class="math notranslate nohighlight">\( \rho \)</span> are 1/2. This implies that <span class="math notranslate nohighlight">\( \rho  = I/2\)</span>. Moreover, this statement generalizes to any dimension <span class="math notranslate nohighlight">\(d  \geq  2\)</span> — for <span class="math notranslate nohighlight">\( \rho  \in D \left( \mathbb{C}^d\right)\)</span> , <span class="math notranslate nohighlight">\(S( \rho )\)</span> is maximized if and only if <span class="math notranslate nohighlight">\( \rho  = I/d\)</span>.</p></li>
<li><p><em>Quantum information is non-negative.</em>
Since <span class="math notranslate nohighlight">\(H(X)  \geq  0\)</span>, it immediately follows by definition that <span class="math notranslate nohighlight">\(S( \rho )  \geq  0\)</span>.</p></li>
<li><p><em>What is the quantum analogue of independent probability distributions <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span></em> ?
Recall that in defining information content, the log function was chosen so as to ensure information is additive when two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independently distributed. The quantum analogue of this has a natural expression: Let <span class="math notranslate nohighlight">\( \rho, \sigma  \in D \left( \mathbb{C}^d\right)  \)</span>, be density matrices. Then, <span class="math notranslate nohighlight">\( \rho \)</span> and <span class="math notranslate nohighlight">\( \sigma \)</span> are independent if their joint state is <span class="math notranslate nohighlight">\( \rho  \otimes  \sigma \)</span>. In the next exercise, you will prove that this indeed preserves our desired additivity property of information for independent quantum states.</p></li>
</ol>
<p>There are other important properties of <span class="math notranslate nohighlight">\(S( \rho )\)</span> which we do not wish to focus on at present; for completeness, however, let us briefly mention two more: (1) For arbitrary, possibly entangled, bipartite mixed states <span class="math notranslate nohighlight">\( \rho_{AB} \)</span>:<span class="math notranslate nohighlight">\(S \left(\rho_{AB}\right) \leq S \left(\rho_{A}\right)+S \left(\rho_{B}\right) \)</span> (subadditivity), and (2) <span class="math notranslate nohighlight">\(S \left( \sum_{i=1}^{n} p_i \rho_i\right) \geq \sum_{i=1}^{n} p_i S \left(\rho_i\right)  \)</span> for <span class="math notranslate nohighlight">\( \left\{p_i\right\}_{i=1}^{m} \)</span> (concavity). Here, and henceforth in this course, we use the shorthand</p>
<div class="math notranslate nohighlight">
\[
 \rho_A :=  \text{Tr}_B\left( \rho_{AB}\right)
\]</div>
</section>
</section>
<section id="send-it-after-class-7">
<h3>Send it after class 7<a class="headerlink" href="#send-it-after-class-7" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>Prove that for any pure state <span class="math notranslate nohighlight">\( \left|\psi\right\rangle \)</span>, <span class="math notranslate nohighlight">\(S( \left| \psi \right\rangle\!\left\langle \psi \right| ) = 0\)</span>.</p></li>
<li><p>For <span class="math notranslate nohighlight">\( \rho =I/d\)</span>, what is <span class="math notranslate nohighlight">\(S( \rho )\)</span>?</p></li>
<li><p>Prove that <span class="math notranslate nohighlight">\(S( \rho  \otimes  \sigma ) = S( \rho ) + S( \sigma )\)</span></p></li>
</ol>
</section>
</section>
<section id="quantifying-entanglement-in-composite-quantum-systems">
<h2>Quantifying entanglement in composite quantum systems<a class="headerlink" href="#quantifying-entanglement-in-composite-quantum-systems" title="Permalink to this headline">¶</a></h2>
<p>With the notion of entropy in hand, we return to the following fundamental question. Let
<span class="math notranslate nohighlight">\( \rho_{AB} \in D \left( \mathbb{C}^d \otimes  \mathbb{C}^d\right)  \)</span> be a bipartite quantum state. Can one efficiently determine if <span class="math notranslate nohighlight">\(\rho_{AB}\)</span> is entangled? (Recall this means that <span class="math notranslate nohighlight">\(\rho_{AB}\)</span> cannot be written <span class="math notranslate nohighlight">\(\rho_{AB} =  \sum_{i=1} p_i  \rho_{A,i}  \otimes  \rho_{B,i}\)</span> as a convex combination of product states.) Roughly, if one uses <span class="math notranslate nohighlight">\(d\)</span> to encode the size of the input (i.e. the input is the entire <span class="math notranslate nohighlight">\(d^2\times d^2\)</span> matrix representing <span class="math notranslate nohighlight">\(\rho_{A,B}\)</span> ), then deciding this question turns out to be NP-hard. This directly implies that quantifying “how much” entanglement is in <span class="math notranslate nohighlight">\(\rho_{A,B}\)</span> is also NP-hard. However, there is a special case in which we <em>can</em> do both tasks efficiently — the case of bipartite <em>pure</em> states <span class="math notranslate nohighlight">\( \left|\psi_{AB}\right\rangle \in  \mathbb{C}^d \otimes  \mathbb{C}^d \)</span> . It is here in which the von Neumann entropy plays a role.</p>
<p>In fact, a previous lecture already discussed an efficient test for entanglement for <span class="math notranslate nohighlight">\(\left|\psi_{AB}\right\rangle\)</span> — the latter is entangled if and only if</p>
<div class="math notranslate nohighlight">
\[
   \rho_A:= \text{Tr}_B\left( \left| \psi_{AB} \right\rangle\!\left\langle \psi_{AB} \right| \right)
  \]</div>
<p>has rank at least 2. This, in turn, followed because it immediately implies the Schmidt rank of <span class="math notranslate nohighlight">\( \left|\psi_{AB}\right\rangle \)</span> is at least 2. However, we can say more. Suppose we have Schmidt decomposition</p>
<div class="math notranslate nohighlight">
\[
   \left|\psi_{AB}\right\rangle= \sum_{i=1}^{d}s_i \left| a_i \right\rangle\!\left\langle b_i \right|
  \]</div>
<p>Then, intuitively, as with the example of a Bell pair, <span class="math notranslate nohighlight">\( \left|\psi_{AB}\right\rangle \)</span> is “highly entangled” if all the Schmidt coefficients <span class="math notranslate nohighlight">\(s_i\)</span> are approximately equal in magnitude, and <span class="math notranslate nohighlight">\( \left|\psi_{AB}\right\rangle \)</span> is “weakly entangled” if there exists a single <span class="math notranslate nohighlight">\(s_i\)</span> whose magnitude is approximately 1. Do we know of a function which quantifies precisely this sort of behavior on the set <span class="math notranslate nohighlight">\(\{s_i\}\)</span>? Indeed, the entropy function! This notion of <span class="math notranslate nohighlight">\(s_i\)</span> being “spread out” versus “concentrated” is highly reminiscent of our fair versus biased coin flip example for the Shannon entropy. We can therefore use the von Neumann entropy to define an entanglement measure <span class="math notranslate nohighlight">\(E( \left|\psi_{AB}\right\rangle )\)</span> as</p>
<div class="math notranslate nohighlight">
\[
  E \left(\left|\psi_{AB}\right\rangle\right):=S \left( \rho_A\right)
  \]</div>
<p>Finally, let us close this section with a natural question — does <span class="math notranslate nohighlight">\(E \left(\left|\psi_{AB}\right\rangle\right)\)</span> still measure entanglement when its input is allowed to be a mixed state <span class="math notranslate nohighlight">\( \rho_{AB} \)</span> (as opposed to a pure state <span class="math notranslate nohighlight">\( \left|\psi_{AB}\right\rangle \)</span>? This is the last question in the following exercise.</p>
<section id="send-it-after-class-8">
<h3>Send it after class 8<a class="headerlink" href="#send-it-after-class-8" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>What is <span class="math notranslate nohighlight">\(E \left(\left|  \Phi^+_{AB}\right\rangle\right) \)</span> for <span class="math notranslate nohighlight">\(  \left|\Phi^+\right\rangle =  \frac{1}{2}  \left( \left|00\right\rangle + \left|11\right\rangle \right)\)</span> a Bell state?</p></li>
<li><p>What is <span class="math notranslate nohighlight">\(E \left( \left|+\right\rangle_A \left|-\right\rangle_B\right) \)</span>?</p></li>
<li><p>Unlike the example of the fair coin, the Schmidt coefficients <span class="math notranslate nohighlight">\(s_i\)</span> of <span class="math notranslate nohighlight">\( \left|\psi_{AB}\right\rangle \)</span> are not probabilities, but amplitudes (i.e. we do not have <span class="math notranslate nohighlight">\( \sum_{i=1}  s_i = 1\)</span>, but rather <span class="math notranslate nohighlight">\( \sum_{i=1}  s_i^2 = 1\)</span>). Show, however, that the notion of probabilities is recovered in the formula <span class="math notranslate nohighlight">\(S( \rho_A)\)</span>, i.e. show that the eigenvalues of <span class="math notranslate nohighlight">\( \rho_A\)</span> are the precisely set <span class="math notranslate nohighlight">\( \left\{s_i^2\right\}_{i=1}^d \)</span>, which <em>do</em> form a distribution.</p></li>
<li><p>Define <span class="math notranslate nohighlight">\(E \left( \rho_{AB} \right):=S \left( \text{Tr}_B\left( \rho_{AB} \right) \right)=S \left( \rho_A \right)\)</span>. Recall that the maximally mixed state on two qubits is a product state, i.e. <span class="math notranslate nohighlight">\(I/4 = I/2  \otimes  I/2\)</span>. Show that <span class="math notranslate nohighlight">\(E(I/4) = 1\)</span>. Why does this imply when cannot use <span class="math notranslate nohighlight">\(E\)</span> as an entanglement measure for bipartite mixed states?</p></li>
</ol>
</section>
</section>
<section id="entanglement-distillation">
<h2>Entanglement distillation<a class="headerlink" href="#entanglement-distillation" title="Permalink to this headline">¶</a></h2>
<p>Now that we have a notion of how to quantify entanglement in pure states, we can become greedy — under what circumstances is it possible to “increase” the amount of entanglement in a composite quantum system? This is a highly non-trivial question, as fundamental communication tasks such as teleportation require highly entangled Bell pairs as a resource. Unfortunately, experimentally producing such pure states is generally a difficult task due to noise from the environment. In other words, in a lab one is typically able to produce <em>mixed</em> states, as opposed to pure states. Moreover, <em>even if</em> Alice could produce perfect Bell pairs in a lab on Earth, when she sends half of a Bell pair to Bob on Mars, the transmitted qubit will again typically be subject to noise, yielding a shared mixed state <span class="math notranslate nohighlight">\( \rho_{AB}\)</span> between Alice and Bob. Do Alice and Bob have any hope of running the teleportation protocol given <span class="math notranslate nohighlight">\( \rho_{AB}\)</span> ?</p>
<p><strong>Local Operations and Classical Communication (LOCC).</strong> To answer the question,it is important to first define the rules of the game. Since Alice and Bob are spatially separated, they are not able to apply joint quantum operations to both systems <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, e.g. they cannot apply a non-factorizable unitary <span class="math notranslate nohighlight">\(U_{AB}  \in  U \left( \mathbb{C}^d \otimes  \mathbb{C}^d\right)\)</span> to <span class="math notranslate nohighlight">\( \rho_{AB}\)</span> . However, they <em>can</em> apply local unitaries and measurements, e.g. factorizable unitaries of the form <span class="math notranslate nohighlight">\(U_A \otimes U_B\)</span> for <span class="math notranslate nohighlight">\(U_A,UB  \in  U( \mathbb{C}^d)\)</span> (i.e.
Alice locally applies <span class="math notranslate nohighlight">\(U_A\)</span> , Bob locally applies <span class="math notranslate nohighlight">\(U_B\)</span> ). They can also pick up the phone and call one another to transmit classical information. Taken together, this set of allowed operations is given a name: Local Operations and Classical Communication (LOCC). The question is thus: <em>Given a shared mixed state <span class="math notranslate nohighlight">\( \rho_{AB}\)</span> , can Alice and Bob use LOCC to “purify” or “distill” Bell states out of <span class="math notranslate nohighlight">\( \rho_{AB}\)</span> ?</em> The answer is sometimes yes, and protocols accomplishing this are called distillation protocols, as they recover “pure” entanglement from “noisy” or mixed state entanglement.</p>
<p><strong>A simple distillation protocol.</strong> We shall discuss a simple distillation protocol, known as the <em>recurrence protocol</em>. Given as input a mixed two-qubit state <span class="math notranslate nohighlight">\( \rho_{AB} \in D \left( \mathbb{C}^2 \otimes  \mathbb{C}^2 \right) \)</span>  our aim is to distill the Bell state known as the <em>singlet</em>,   <span class="math notranslate nohighlight">\(\left|\Psi^-\right\rangle =  \frac{1}{\sqrt{2}}  \left( \left|01\right\rangle - \left|10\right\rangle \right)\)</span>  ; note <span class="math notranslate nohighlight">\(I \otimes Y \left|\Psi^-\right\rangle =i \left|\Phi^+\right\rangle \)</span>, making it easy to convert one Bell state into the other for the teleportation scheme. There is a required precondition for the protocol to work — the input state <span class="math notranslate nohighlight">\( \rho_{AB}\)</span> must have sufficient initial overlap with <span class="math notranslate nohighlight">\( \left|\Psi^-\right\rangle \)</span> i, i.e.</p>
<div class="math notranslate nohighlight">
\[
F ( \rho_{AB} ) :=  \left\langle \Psi^- \right| \rho_{AB}\left| \Psi^- \right\rangle  &gt; \frac{1}{2}
\]</div>
<p>In other words, in transmitting half of <span class="math notranslate nohighlight">\( \left|\Psi^-\right\rangle \)</span> from Alice to Bob, the resulting mixed state <span class="math notranslate nohighlight">\( \rho_{AB}\)</span> should not have deviated “too far” from <span class="math notranslate nohighlight">\( \left|\Psi^-\right\rangle \)</span>. Henceforth, we shall use shorthand <span class="math notranslate nohighlight">\(F\)</span> to denote <span class="math notranslate nohighlight">\(F ( \rho_{AB} )\)</span> for brevity.</p>
<p>Suppose Alice and Bob share two copies of <span class="math notranslate nohighlight">\( \rho_{AB}\)</span> ; let us label them <span class="math notranslate nohighlight">\( \rho_{A_1B_1}\)</span> and <span class="math notranslate nohighlight">\( \rho_{A_2B_2}\)</span> , where Alice holds systems <span class="math notranslate nohighlight">\(A_1 , A_2\)</span> , and Bob holds <span class="math notranslate nohighlight">\(B_1 , B_2\)</span> . Each round of the distillation protocol proceeds as follows.</p>
<ol>
<li><p>(Twirling operation) Alice picks a Pauli operator <span class="math notranslate nohighlight">\(U\)</span> from set <span class="math notranslate nohighlight">\(\{I, X, Y, Z\}\)</span> uniformly at random, and communicates this choice to Bob. They each locally apply operator <span class="math notranslate nohighlight">\(\sqrt{U}\)</span> to <span class="math notranslate nohighlight">\( \rho_{A_iB_i}\)</span> for <span class="math notranslate nohighlight">\(i  \in  \{1, 2\}\)</span> (note that <span class="math notranslate nohighlight">\(U\)</span> is defined using the notion of operator functions), obtaining</p>
<div class="math notranslate nohighlight">
\[
    \sigma_{A_iB_i}:= \left(\sqrt{U_A} \otimes\sqrt{U_B} \right) \rho_{A_iB_i}\left(\sqrt{U_A}^\dagger  \otimes\sqrt{U_B}^\dagger  \right)
   \]</div>
<p>This random choice of Pauli and its subsequent local application is together called the <em>twirling map</em> <span class="math notranslate nohighlight">\(\Phi:D \left( \mathbb{C}^2 \otimes  \mathbb{C}^2\right) \mapsto D \left( \mathbb{C}^2 \otimes  \mathbb{C}^2\right) \)</span>, and is not a unitary map (due to the random choice over Pauli operators); nevertheless, it can clearly be implemented given the ability to flip random coins and apply arbitrary single qubit gates. (The formal framework for studying such operations is via <em>Trace Preserving Completely Positive Maps</em>, and is beyond the scope of this course.) The nice thing about the twirling operation is that, for any input <span class="math notranslate nohighlight">\( \rho_{AB}\)</span> , <span class="math notranslate nohighlight">\(\Phi \left(\rho_{AB}\right) \)</span> can be diagonalized in the Bell basis, i.e. can be written</p>
<div class="math notranslate nohighlight">
\[
    \Phi \left(\rho_{AB}\right)  = F  \left| \Psi^- \right\rangle\!\left\langle \Psi^- \right|+  \frac{1-F}{3} \left| \Psi^+ \right\rangle\!\left\langle \Psi^+ \right| +  \frac{1-F}{3} \left| \Phi^+ \right\rangle\!\left\langle \Phi^+ \right| + \frac{1-F}{3} \left| \Phi^- \right\rangle\!\left\langle \Phi^- \right|
    \]</div>
<p>for Bell basis <span class="math notranslate nohighlight">\( \left\{ \left|\Phi^+\right\rangle , \left|\Phi^-\right\rangle , \left|\Psi^+\right\rangle , \left|\Psi^-\right\rangle \right\} \)</span>, where <span class="math notranslate nohighlight">\(F\)</span> is given above</p>
</li>
<li><p>(Convert from <span class="math notranslate nohighlight">\( \left|\Psi^-\right\rangle \)</span> to <span class="math notranslate nohighlight">\( \left|\Phi^+\right\rangle \)</span> Alice applies Pauli <span class="math notranslate nohighlight">\(Y\)</span> to her half of each state to obtain states:</p>
<div class="math notranslate nohighlight">
\[
    \sigma_{A_iB_i}=\frac{1-F}{3} \left| \Psi^- \right\rangle\!\left\langle \Psi^- \right| +  \frac{1-F}{3} \left| \Psi^+ \right\rangle\!\left\langle \Psi^+ \right| + F \left| \Phi^+ \right\rangle\!\left\langle \Phi^+ \right| +\frac{1-F}{3} \left| \Phi^- \right\rangle\!\left\langle \Phi^- \right|
   \]</div>
<p>This shifts most of the weight in <span class="math notranslate nohighlight">\(\Phi( \rho_{A_iB_i} \)</span> from <span class="math notranslate nohighlight">\( \left|\Psi^-\right\rangle \)</span> to <span class="math notranslate nohighlight">\( \left|\Phi^+\right\rangle \)</span>, since <span class="math notranslate nohighlight">\(F &gt; 1/2\)</span>.</p>
</li>
<li><p>(Application of CNOT gates) Alice applies a CNOT gate with qubit <span class="math notranslate nohighlight">\(A_1\)</span> as the control and <span class="math notranslate nohighlight">\(A_2\)</span> as the target. Bob does the same for <span class="math notranslate nohighlight">\(B_1\)</span> and <span class="math notranslate nohighlight">\(B_2\)</span> .</p></li>
<li><p>(Local measurements) Alice and Bob each locally measure <span class="math notranslate nohighlight">\(A_2\)</span> and <span class="math notranslate nohighlight">\(B_2\)</span> in the standard basis, obtaining outcomes <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> in <span class="math notranslate nohighlight">\(\{0, 1\}\)</span>, respectively. They pick up the phone to compare their measurement results <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. If <span class="math notranslate nohighlight">\(a = b\)</span>, they keep the remaining composite system on <span class="math notranslate nohighlight">\(AB\)</span>, denoted <span class="math notranslate nohighlight">\( \sigma^\prime_{A_1B_1}  \)</span>. Otherwise if <span class="math notranslate nohighlight">\(a \neq b\)</span>, they throw out all systems and start again.</p></li>
<li><p>(Convert from <span class="math notranslate nohighlight">\( \left|\Phi^+\right\rangle \)</span> to <span class="math notranslate nohighlight">\( \left|\Psi^-\right\rangle \)</span>) Alice applies Pauli <span class="math notranslate nohighlight">\(Y\)</span> to her half of <span class="math notranslate nohighlight">\(\sigma^\prime_{A_1B_1}\)</span> to convert its <span class="math notranslate nohighlight">\( \left|\Phi^+\right\rangle \)</span> component back to <span class="math notranslate nohighlight">\( \left|\Psi^-\right\rangle \)</span>.</p></li>
</ol>
<p>To get a better sense of this protocol in action, let us apply it to a concrete example. Suppose Alice sends half of the singlet to Bob, and along the way, the state <span class="math notranslate nohighlight">\( \left|\Psi^-\right\rangle \)</span> is injected with some completely random noise, denoted by the identity matrix:</p>
<div class="math notranslate nohighlight">
\[
 \rho_{AB}=  \frac{1}{2}  \left| \Psi^- \right\rangle\!\left\langle \Psi^- \right| +  \frac{1}{8} I = \frac{3}{4} \left| \Psi^- \right\rangle\!\left\langle \Psi^- \right| + \frac{1}{12} \left| \Psi^+ \right\rangle\!\left\langle \Psi^+ \right| + \frac{1}{12} \left| \Phi^+ \right\rangle\!\left\langle \Phi^+ \right| + \frac{1}{12} \left| \Phi^- \right\rangle\!\left\langle \Phi^- \right|
\]</div>
<p>where the second equality follows by recalling the identity matrix diagonalizes in any basis, including the Bell basis. (The noise-inducing channel above is formally known as the <em>depolarizing channel</em> in quantum information theory.)</p>
<p>Steps 3 and 4 are a bit messier. The intuition here is that the CNOT entangles <span class="math notranslate nohighlight">\( \sigma_{A_1B_1}\)</span> with <span class="math notranslate nohighlight">\( \sigma_{A_2B_2}\)</span> , and the measurement forces the bits in the second system (formerly in state <span class="math notranslate nohighlight">\( \sigma_{A_2B_2}\)</span> ) to match; via the entanglement just created, this increases the weight on the
terms where bits match, i.e. <span class="math notranslate nohighlight">\( \left| \Phi^+ \right\rangle\!\left\langle \Phi^+ \right| \)</span> and <span class="math notranslate nohighlight">\( \left| \Phi^- \right\rangle\!\left\langle \Phi^- \right| \)</span>. Thus, the final Step 5 will yield a state with higher overlap on the desired singlet state <span class="math notranslate nohighlight">\( \left|\Psi^-\right\rangle \)</span>.</p>
<p>To run through the full technical analysis for Steps 3 and 4 would be tedious, so we analyze one term of the computation for brevity. Before Step 3 is run, Alice and Bob share state</p>
<div class="math notranslate nohighlight">
\[
 \sigma_{A_1B_1} \otimes \sigma_{A_2B_2} \in D \left( \mathbb{C}^2 \otimes \mathbb{C}^2 \otimes\mathbb{C}^2 \otimes\mathbb{C}^2 \right)
\]</div>
<p>where recall Alice holds qubits <span class="math notranslate nohighlight">\(A_1\)</span> , <span class="math notranslate nohighlight">\(A_2\)</span> , and Bob holds <span class="math notranslate nohighlight">\(B_1\)</span> , <span class="math notranslate nohighlight">\(B_2\)</span> . Since each <span class="math notranslate nohighlight">\(\sigma_{A_iB_i}\)</span> has 4 terms in its mixture, the tensor product has 16 terms. By linearity, Step 3 then applies gates CNOT<span class="math notranslate nohighlight">\(_{A_1A_2}\)</span> and CNOT<span class="math notranslate nohighlight">\(_{B_1B_2}\)</span> to each of these 16 terms, where our notational convention is that CNOT<span class="math notranslate nohighlight">\(_{12}\)</span> has qubit 1 as the control and qubit 2 as the target.</p>
<p>Finally, let us briefly state what this protocol does. A careful but tedious analysis yields that with probability at least 1/4, this protocol maps the input state <span class="math notranslate nohighlight">\( \rho_{A_1B_1} \otimes \rho_{A_2B_2}\)</span> to an output state <span class="math notranslate nohighlight">\( \sigma^\prime_{A_1B_1} \)</span> such that (for <span class="math notranslate nohighlight">\(F_\rho  := F ( \rho_{A_1B_1} )\)</span>)</p>
<div class="math notranslate nohighlight">
\[
F (  \sigma^\prime_{A_1B_1} )= \frac{F_\rho^2+ \frac{1}{9} \left(1-F_\rho\right)^2  }{F_\rho^2+ \frac{2}{3}F_\rho \left(1-F_\rho\right)+ \frac{5}{9} \left(1-F_\rho\right)^2    }
\]</div>
<p>So long as <span class="math notranslate nohighlight">\(F_\rho &gt; 1/2\)</span>, one can show <span class="math notranslate nohighlight">\(F (  \sigma^\prime_{A_1B_1} )&gt;F_\rho\)</span> ; thus, recursively applying this protocol (using many pairs of input states <span class="math notranslate nohighlight">\( \rho_{AB}\)</span> ) improves our overlap with our desired target state of <span class="math notranslate nohighlight">\( \left|\Psi^-\right\rangle \)</span>.</p>
<section id="send-it-after-class-9">
<h3>Send it after class 9<a class="headerlink" href="#send-it-after-class-9" title="Permalink to this headline">¶</a></h3>
<ol>
<li><p>Show that <span class="math notranslate nohighlight">\(\sqrt{Z} \otimes \sqrt{Z}\)</span> maps <span class="math notranslate nohighlight">\( \left|\Phi^+\right\rangle \)</span> to <span class="math notranslate nohighlight">\( \left|\Phi^-\right\rangle \)</span> and vice  versa.  Using the additional identities that <span class="math notranslate nohighlight">\(\sqrt{X}  \otimes  \sqrt{X}\)</span> maps <span class="math notranslate nohighlight">\( \left|\Phi^+\right\rangle \)</span> to <span class="math notranslate nohighlight">\( \left|\Psi^+\right\rangle \)</span> and vice versa, and <span class="math notranslate nohighlight">\(\sqrt{Y}  \otimes  \sqrt{Y}\)</span> maps <span class="math notranslate nohighlight">\( \left|\Phi^-\right\rangle \)</span> to <span class="math notranslate nohighlight">\( \left|\Psi^+\right\rangle \)</span> and vice versa, show that the twirling map leaves <span class="math notranslate nohighlight">\(\rho_{AB}=  \frac{1}{2}  \left| \Psi^- \right\rangle\!\left\langle \Psi^- \right| +  \frac{1}{8} I = \frac{3}{4} \left| \Psi^- \right\rangle\!\left\langle \Psi^- \right| + \frac{1}{12} \left| \Psi^+ \right\rangle\!\left\langle \Psi^+ \right| + \frac{1}{12} \left| \Phi^+ \right\rangle\!\left\langle \Phi^+ \right| + \frac{1}{12} \left| \Phi^- \right\rangle\!\left\langle \Phi^- \right|\)</span> invariant.</p></li>
<li><p>Show that applying <span class="math notranslate nohighlight">\(Y_A \otimes I\)</span> to <span class="math notranslate nohighlight">\( \rho_{AB}\)</span> yields in Step 2 that</p>
<div class="math notranslate nohighlight">
\[
    \sigma_{AB}= \frac{1}{12} \left| \Psi^- \right\rangle\!\left\langle \Psi^- \right| +  \frac{1}{12} \left| \Psi^+ \right\rangle\!\left\langle \Psi^+ \right| + \frac{3}{4} \left| \Phi^+ \right\rangle\!\left\langle \Phi^+ \right| + \frac{1}{12} \left| \Phi^- \right\rangle\!\left\langle \Phi^- \right|
   \]</div>
</li>
<li><p>Let us analyze one of these 16 terms: <span class="math notranslate nohighlight">\( \left| \Phi^+ \right\rangle\!\left\langle \Phi^+ \right|_{A_1B_1} \otimes  \left| \Phi^+ \right\rangle\!\left\langle \Phi^+ \right|_{A_2B_2}\)</span> .Show that</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
  (\text{CNOT}_{A_1A_2}  \otimes  \text{CNOT}_{B_1B_2} ) \left|\Phi^+\right\rangle_{A_1B_1} \otimes \left|\Phi^+\right\rangle_{A_2B_2} =  \frac{1}{2}  \left( \left|0000\right\rangle+\left|0011\right\rangle+\left|1100\right\rangle +\left|1111\right\rangle\right)_{A_1B_1A_2B_2}
  \]</div>
<p>If Alice and Bob run Step 4 on this state and obtain matching outcomes <span class="math notranslate nohighlight">\(a = b\)</span>, what does the state on qubits <span class="math notranslate nohighlight">\(A_1B_1\)</span> collapse to?</p>
</section>
</section>
</section>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../../../../_sources/courses/PHYS437/Theory/Ch2/Theory-Lecture7.ipynb.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright (CC BY 3.0) https://creativecommons.org/ .<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.4.0.<br/>
    </p>
  </div>
</footer>
  </body>
</html>
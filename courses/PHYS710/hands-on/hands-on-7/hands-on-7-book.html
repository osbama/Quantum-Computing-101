<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Training and evaluating quantum kernels &#8212; Practical Quantum Computing for Scientists 2022.02.24 alpha documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/bootstrap-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/sphinx_highlight.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Archives" href="../../../archives/archives.html" />
    <link rel="prev" title="Hands-on session 7" href="hands-on-7.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../../../_static/js/jquery-1.12.4.min.js"></script>
<script type="text/javascript" src="../../../../_static/js/jquery-fix.js"></script>
<script type="text/javascript" src="../../../../_static/bootstrap-3.4.1/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../../../../_static/bootstrap-sphinx.js"></script>

  </head><body>

  <div id="navbar" class="navbar navbar-inverse navbar-default ">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../index.html">
          Practical QC for Scientists</a>
        <span class="navbar-text navbar-version pull-left"><b>2022.02.24</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../../../PHYS437/index.html">437</a></li>
                <li><a href="../../index.html">710</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../../index.html">Contents <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../index.html">Courses</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../PHYS437/index.html">PHYS 437</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../index.html">PHYS 710</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../archives/archives.html">Archives</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../help/index.html">HOWTOs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../help/IBM_quantum.html">Using IBM quantum Cloud</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This cell is added by sphinx-gallery</span>
<span class="c1"># It can be customized to whatever you like</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="training-and-evaluating-quantum-kernels">
<h1>Training and evaluating quantum kernels<a class="headerlink" href="#training-and-evaluating-quantum-kernels" title="Permalink to this heading">¶</a></h1>
<p>Kernel methods are one of the cornerstones of classical machine learning. Here we are concerned with kernels that can be evaluated on quantum computers, <em>quantum kernels</em> for short. In this tutorial you will learn how to evaluate kernels, use them for classification and train them with gradient-based optimization, and all that using the functionality of PennyLane’s <a class="reference external" href="https://pennylane.readthedocs.io/en/latest/code/qml_kernels.html">kernels module</a>.</p>
<section id="what-are-kernel-methods">
<h2>What are kernel methods?<a class="headerlink" href="#what-are-kernel-methods" title="Permalink to this heading">¶</a></h2>
<p>To understand what a kernel method does, let’s first revisit one of the simplest methods to assign binary labels to datapoints: linear classification.</p>
<p>Imagine we want to discern two different classes of points that lie in different corners of the plane. A linear classifier corresponds to drawing a line and assigning different labels to the regions on opposing
sides of the line:</p>
<p><img alt="" src="../../../../_images/linear_classification.png" /></p>
<p>We can mathematically formalize this by assigning the label <span class="math notranslate nohighlight">\(y\)</span> via</p>
<div class="math notranslate nohighlight">
\[y(\boldsymbol{x}) = \operatorname{sgn}(\langle \boldsymbol{w}, \boldsymbol{x}\rangle + b).\]</div>
<p>The vector <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> points perpendicular to the line and thus determine its slope. The independent term <span class="math notranslate nohighlight">\(b\)</span> specifies the position on the plane. In this form, linear classification can also be extended to higher dimensional vectors <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, where a line does not divide the entire space into two regions anymore. Instead one needs a <em>hyperplane</em>. It is immediately clear that this method is not very powerful, as datasets that are not separable by a hyperplane can’t be classified without error.</p>
<p>We can actually sneak around this limitation by performing a neat trick: if we define some map <span class="math notranslate nohighlight">\(\phi(\boldsymbol{x})\)</span> that <em>embeds</em> our datapoints into a larger <em>feature space</em> and then perform linear
classification there, we could actually realise non-linear classification in our original space!</p>
<p><img alt="" src="../../../../_images/embedding_nonlinear_classification.png" /></p>
<p>If we go back to the expression for our prediction and include the embedding, we get</p>
<div class="math notranslate nohighlight">
\[y(\boldsymbol{x}) = \operatorname{sgn}(\langle \boldsymbol{w}, \phi(\boldsymbol{x})\rangle + b).\]</div>
<p>We will forgo one tiny step, but it can be shown that for the purpose of optimal classification, we can choose the vector defining the decision boundary as a linear combination of the embedded datapoints
<span class="math notranslate nohighlight">\(\boldsymbol{w} = \sum_i \alpha_i \phi(\boldsymbol{x}_i)\)</span>. Putting this into the formula yields</p>
<div class="math notranslate nohighlight">
\[y(\boldsymbol{x}) = \operatorname{sgn}\left(\sum_i \alpha_i \langle \phi(\boldsymbol{x}_i), \phi(\boldsymbol{x})\rangle + b\right).\]</div>
<p>This rewriting might not seem useful at first, but notice the above formula only contains inner products between vectors in the embedding space:</p>
<div class="math notranslate nohighlight">
\[k(\boldsymbol{x}_i, \boldsymbol{x}_j) = \langle \phi(\boldsymbol{x}_i), \phi(\boldsymbol{x}_j)\rangle.\]</div>
<p>We call this function the <em>kernel</em>. It provides the advantage that we can often find an explicit formula for the kernel <span class="math notranslate nohighlight">\(k\)</span> that makes it superfluous to actually perform the (potentially expensive) embedding <span class="math notranslate nohighlight">\(\phi\)</span>. Consider for example the following embedding and the associated kernel:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\phi((x_1, x_2)) &amp;= (x_1^2, \sqrt{2} x_1 x_2, x_2^2) \\
k(\boldsymbol{x}, \boldsymbol{y}) &amp;= x_1^2 y_1^2 + 2 x_1 x_2 y_1 y_2 + x_2^2 y_2^2 = \langle \boldsymbol{x}, \boldsymbol{y} \rangle^2.
\end{aligned}\end{split}\]</div>
<p>This means by just replacing the regular scalar product in our linear classification with the map <span class="math notranslate nohighlight">\(k\)</span>, we can actually express much more intricate decision boundaries!</p>
<p>This is very important, because in many interesting cases the embedding <span class="math notranslate nohighlight">\(\phi\)</span> will be much costlier to compute than the kernel <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>In this demo, we will explore one particular kind of kernel that can be realized on near-term quantum computers, namely <em>Quantum Embedding Kernels (QEKs)</em>. These are kernels that arise from embedding data into
the space of quantum states. We formalize this by considering a parameterised quantum circuit <span class="math notranslate nohighlight">\(U(\boldsymbol{x})\)</span> that maps a datapoint <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> to the state</p>
<div class="math notranslate nohighlight">
\[|\psi(\boldsymbol{x})\rangle = U(\boldsymbol{x}) |0 \rangle.\]</div>
<p>The kernel value is then given by the <em>overlap</em> of the associated embedded quantum states</p>
<div class="math notranslate nohighlight">
\[k(\boldsymbol{x}_i, \boldsymbol{x}_j) = | \langle\psi(\boldsymbol{x}_i)|\psi(\boldsymbol{x}_j)\rangle|^2.\]</div>
</section>
<section id="a-toy-problem">
<h2>A toy problem<a class="headerlink" href="#a-toy-problem" title="Permalink to this heading">¶</a></h2>
<p>In this demo, we will treat a toy problem that showcases the inner workings of classification with quantum embedding kernels, training variational embedding kernels and the available functionalities to do both in PennyLane. We of course need to start with some imports:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pennylane</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1359</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And we proceed right away to create a dataset to work with, the <code class="docutils literal notranslate"><span class="pre">DoubleCake</span></code> dataset. Firstly, we define two functions to enable us to generate the data. The details of these functions are not essential for understanding the demo, so don’t mind them if they are confusing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_make_circular_data</span><span class="p">(</span><span class="n">num_sectors</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generate datapoints arranged in an even circle.&quot;&quot;&quot;</span>
    <span class="n">center_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_sectors</span><span class="p">))</span>
    <span class="n">sector_angle</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="n">num_sectors</span>
    <span class="n">angles</span> <span class="o">=</span> <span class="p">(</span><span class="n">center_indices</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">sector_angle</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mf">0.7</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="mf">0.7</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor_divide</span><span class="p">(</span><span class="n">angles</span><span class="p">,</span> <span class="n">sector_angle</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">labels</span>


<span class="k">def</span> <span class="nf">make_double_cake_data</span><span class="p">(</span><span class="n">num_sectors</span><span class="p">):</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">labels1</span> <span class="o">=</span> <span class="n">_make_circular_data</span><span class="p">(</span><span class="n">num_sectors</span><span class="p">)</span>
    <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">labels2</span> <span class="o">=</span> <span class="n">_make_circular_data</span><span class="p">(</span><span class="n">num_sectors</span><span class="p">)</span>

    <span class="c1"># x and y coordinates of the datapoints</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x2</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">y1</span><span class="p">,</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">y2</span><span class="p">])</span>

    <span class="c1"># Canonical form of dataset</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>

    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">labels1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">labels2</span><span class="p">])</span>

    <span class="c1"># Canonical form of labels</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we define a function to help plot the <code class="docutils literal notranslate"><span class="pre">DoubleCake</span></code> data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_double_cake_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">num_sectors</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Plot double cake data and corresponding sectors.&quot;&quot;&quot;</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;#FF0000&quot;</span><span class="p">,</span> <span class="s2">&quot;#0000FF&quot;</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;s&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">num_sectors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sector_angle</span> <span class="o">=</span> <span class="mi">360</span> <span class="o">/</span> <span class="n">num_sectors</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_sectors</span><span class="p">):</span>
            <span class="n">color</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#FF0000&quot;</span><span class="p">,</span> <span class="s2">&quot;#0000FF&quot;</span><span class="p">][(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">)]</span>
            <span class="n">other_color</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#FF0000&quot;</span><span class="p">,</span> <span class="s2">&quot;#0000FF&quot;</span><span class="p">][((</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span><span class="p">)]</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span>
                <span class="n">mpl</span><span class="o">.</span><span class="n">patches</span><span class="o">.</span><span class="n">Wedge</span><span class="p">(</span>
                    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                    <span class="mi">1</span><span class="p">,</span>
                    <span class="n">i</span> <span class="o">*</span> <span class="n">sector_angle</span><span class="p">,</span>
                    <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">sector_angle</span><span class="p">,</span>
                    <span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span>
                    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                    <span class="n">width</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span>
                <span class="n">mpl</span><span class="o">.</span><span class="n">patches</span><span class="o">.</span><span class="n">Wedge</span><span class="p">(</span>
                    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                    <span class="mf">0.5</span><span class="p">,</span>
                    <span class="n">i</span> <span class="o">*</span> <span class="n">sector_angle</span><span class="p">,</span>
                    <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">sector_angle</span><span class="p">,</span>
                    <span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="n">color</span><span class="o">=</span><span class="n">other_color</span><span class="p">,</span>
                    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ax</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s now have a look at our dataset. In our example, we will work with
3 sectors:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">num_sectors</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">make_double_cake_data</span><span class="p">(</span><span class="n">num_sectors</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_double_cake_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">(),</span> <span class="n">num_sectors</span><span class="o">=</span><span class="n">num_sectors</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/29deb60806af528347236142a940b949842622dd9cfd2107452cd8c3915ec73a.png" src="../../../../_images/29deb60806af528347236142a940b949842622dd9cfd2107452cd8c3915ec73a.png" />
</div>
</div>
</section>
<section id="defining-a-quantum-embedding-kernel">
<h2>Defining a Quantum Embedding Kernel<a class="headerlink" href="#defining-a-quantum-embedding-kernel" title="Permalink to this heading">¶</a></h2>
<p>PennyLane’s <a class="reference external" href="https://pennylane.readthedocs.io/en/latest/code/qml_kernels.html">kernels module</a> allows for a particularly simple implementation of Quantum Embedding Kernels. The first ingredient we need for this is an <em>ansatz</em>, which we will construct by repeating a layer as building block. Let’s start by defining this layer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pennylane</span> <span class="k">as</span> <span class="nn">qml</span>


<span class="k">def</span> <span class="nf">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">wires</span><span class="p">,</span> <span class="n">i0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">inc</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Building block of the embedding ansatz&quot;&quot;&quot;</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">i0</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">wire</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">wires</span><span class="p">):</span>
        <span class="n">qml</span><span class="o">.</span><span class="n">Hadamard</span><span class="p">(</span><span class="n">wires</span><span class="o">=</span><span class="p">[</span><span class="n">wire</span><span class="p">])</span>
        <span class="n">qml</span><span class="o">.</span><span class="n">RZ</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)],</span> <span class="n">wires</span><span class="o">=</span><span class="p">[</span><span class="n">wire</span><span class="p">])</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="n">inc</span>
        <span class="n">qml</span><span class="o">.</span><span class="n">RY</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">wires</span><span class="o">=</span><span class="p">[</span><span class="n">wire</span><span class="p">])</span>

    <span class="n">qml</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">unitary</span><span class="o">=</span><span class="n">qml</span><span class="o">.</span><span class="n">CRZ</span><span class="p">,</span> <span class="n">pattern</span><span class="o">=</span><span class="s2">&quot;ring&quot;</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="n">wires</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>To construct the ansatz, this layer is repeated multiple times, reusing the datapoint <code class="docutils literal notranslate"><span class="pre">x</span></code> but feeding different variational parameters <code class="docutils literal notranslate"><span class="pre">params</span></code> into each of them. Together, the datapoint and the variational
parameters fully determine the embedding ansatz <span class="math notranslate nohighlight">\(U(\boldsymbol{x})\)</span>. In order to construct the full kernel circuit, we also require its adjoint <span class="math notranslate nohighlight">\(U(\boldsymbol{x})^\dagger\)</span>, which we can obtain via <code class="docutils literal notranslate"><span class="pre">qml.adjoint</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ansatz</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">wires</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The embedding ansatz&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">layer_params</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">layer_params</span><span class="p">,</span> <span class="n">wires</span><span class="p">,</span> <span class="n">i0</span><span class="o">=</span><span class="n">j</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">wires</span><span class="p">))</span>


<span class="n">adjoint_ansatz</span> <span class="o">=</span> <span class="n">qml</span><span class="o">.</span><span class="n">adjoint</span><span class="p">(</span><span class="n">ansatz</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">random_params</span><span class="p">(</span><span class="n">num_wires</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generate random variational parameters in the shape for the ansatz.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_wires</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Together with the ansatz we only need a device to run the quantum circuit on. For the purpose of this tutorial we will use PennyLane’s <code class="docutils literal notranslate"><span class="pre">default.qubit</span></code> device with 5 wires in analytic mode.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dev</span> <span class="o">=</span> <span class="n">qml</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;default.qubit&quot;</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shots</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">wires</span> <span class="o">=</span> <span class="n">dev</span><span class="o">.</span><span class="n">wires</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Let us now define the quantum circuit that realizes the kernel. We will compute the overlap of the quantum states by first applying the embedding of the first datapoint and then the adjoint of the embedding
of the second datapoint. We finally extract the probabilities of observing each basis state.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@qml</span><span class="o">.</span><span class="n">qnode</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">kernel_circuit</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="n">ansatz</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="n">wires</span><span class="p">)</span>
    <span class="n">adjoint_ansatz</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="n">wires</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">qml</span><span class="o">.</span><span class="n">probs</span><span class="p">(</span><span class="n">wires</span><span class="o">=</span><span class="n">wires</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The kernel function itself is now obtained by looking at the probability of observing the all-zero state at the end of the kernel circuit – because of the ordering in <code class="docutils literal notranslate"><span class="pre">qml.probs</span></code>, this is the first entry:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">kernel_circuit</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">params</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Note</p>
<p>An alternative way to set up the kernel circuit in PennyLane would be to use the observable type <a class="reference external" href="https://pennylane.readthedocs.io/en/latest/code/api/pennylane.Projector.html">Projector</a>.
This is shown in the <a class="reference external" href="https://pennylane.ai/qml/demos/tutorial_kernel_based_training.html">demo on kernel-based training of quantum models</a>, where you will also find more background information on the kernel circuit structure itself.</p>
<p>Before focusing on the kernel values we have to provide values for the
variational parameters. At this point we fix the number of layers in the
ansatz circuit to <span class="math notranslate nohighlight">\(6\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">init_params</span> <span class="o">=</span> <span class="n">random_params</span><span class="p">(</span><span class="n">num_wires</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can have a look at the kernel value between the first and the
second datapoint:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kernel_value</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">init_params</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The kernel value between the first and second datapoint is </span><span class="si">{</span><span class="n">kernel_value</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The kernel value between the first and second datapoint is 0.093
</pre></div>
</div>
</div>
</div>
<p>The mutual kernel values between all elements of the dataset form the
<em>kernel matrix</em>. We can inspect it via the
<code class="docutils literal notranslate"><span class="pre">qml.kernels.square_kernel_matrix</span></code> method, which makes use of symmetry
of the kernel,
<span class="math notranslate nohighlight">\(k(\boldsymbol{x}_i,\boldsymbol{x}_j) = k(\boldsymbol{x}_j, \boldsymbol{x}_i)\)</span>.
In addition, the option <code class="docutils literal notranslate"><span class="pre">assume_normalized_kernel=True</span></code> ensures that we
do not calculate the entries between the same datapoints, as we know
them to be 1 for our noiseless simulation. Overall this means that we
compute <span class="math notranslate nohighlight">\(\frac{1}{2}(N^2-N)\)</span> kernel values for <span class="math notranslate nohighlight">\(N\)</span> datapoints. To
include the variational parameters, we construct a <code class="docutils literal notranslate"><span class="pre">lambda</span></code> function
that fixes them to the values we sampled above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">init_kernel</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">:</span> <span class="n">kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">init_params</span><span class="p">)</span>
<span class="n">K_init</span> <span class="o">=</span> <span class="n">qml</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">square_kernel_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">init_kernel</span><span class="p">,</span> <span class="n">assume_normalized_kernel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">K_init</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1.    0.093 0.012 0.721 0.149 0.055]
 [0.093 1.    0.056 0.218 0.73  0.213]
 [0.012 0.056 1.    0.032 0.191 0.648]
 [0.721 0.218 0.032 1.    0.391 0.226]
 [0.149 0.73  0.191 0.391 1.    0.509]
 [0.055 0.213 0.648 0.226 0.509 1.   ]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="using-the-quantum-embedding-kernel-for-predictions">
<h2>Using the Quantum Embedding Kernel for predictions<a class="headerlink" href="#using-the-quantum-embedding-kernel-for-predictions" title="Permalink to this heading">¶</a></h2>
<p>The quantum kernel alone can not be used to make predictions on a
dataset, becaues it is essentially just a tool to measure the similarity
between two datapoints. To perform an actual prediction we will make use
of scikit-learn’s Support Vector Classifier (SVC).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
</pre></div>
</div>
</div>
</div>
<p>To construct the SVM, we need to supply <code class="docutils literal notranslate"><span class="pre">sklearn.svm.SVC</span></code> with a function that takes two sets of datapoints and returns the associated kernel matrix. We can make use of the function <code class="docutils literal notranslate"><span class="pre">qml.kernels.kernel_matrix</span></code> that provides this functionality. It expects the kernel to not have additional parameters besides the datapoints, which is why we again supply the variational parameters via the <code class="docutils literal notranslate"><span class="pre">lambda</span></code> function from above. Once we have this, we can let scikit-learn adjust the SVM from our Quantum Embedding Kernel.</p>
<p>Note:
This step does <em>not</em> modify the variational parameters in our circuit ansatz. What it does is solving a different optimization task for the <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(b\)</span> vectors we introduced in the beginning.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svm</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="k">lambda</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">:</span> <span class="n">qml</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">kernel_matrix</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">init_kernel</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To see how well our classifier performs we will measure which percentage of the dataset it classifies correctly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y_target</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">Y_target</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_target</span><span class="p">)</span>


<span class="n">accuracy_init</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">svm</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The accuracy of the kernel with random parameters is </span><span class="si">{</span><span class="n">accuracy_init</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The accuracy of the kernel with random parameters is 0.833
</pre></div>
</div>
</div>
</div>
<p>We are also interested in seeing what the decision boundaries in this classification look like. This could help us spotting overfitting issues visually in more complex data sets. To this end we will introduce a second helper method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_decision_boundaries</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">N_gridpoints</span><span class="o">=</span><span class="mi">14</span><span class="p">):</span>
    <span class="n">_xx</span><span class="p">,</span> <span class="n">_yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N_gridpoints</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N_gridpoints</span><span class="p">))</span>

    <span class="n">_zz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">_xx</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">ndindex</span><span class="p">(</span><span class="o">*</span><span class="n">_xx</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
        <span class="n">_zz</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">_xx</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">_yy</span><span class="p">[</span><span class="n">idx</span><span class="p">]])[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:])</span>

    <span class="n">plot_data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;_xx&quot;</span><span class="p">:</span> <span class="n">_xx</span><span class="p">,</span> <span class="s2">&quot;_yy&quot;</span><span class="p">:</span> <span class="n">_yy</span><span class="p">,</span> <span class="s2">&quot;_zz&quot;</span><span class="p">:</span> <span class="n">_zz</span><span class="p">}</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span>
        <span class="n">_xx</span><span class="p">,</span>
        <span class="n">_yy</span><span class="p">,</span>
        <span class="n">_zz</span><span class="p">,</span>
        <span class="n">cmap</span><span class="o">=</span><span class="n">mpl</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;#FF0000&quot;</span><span class="p">,</span> <span class="s2">&quot;#0000FF&quot;</span><span class="p">]),</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="n">plot_double_cake_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">plot_data</span>
</pre></div>
</div>
</div>
</div>
<p>With that done, let’s have a look at the decision boundaries for our initial classifier:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">init_plot_data</span> <span class="o">=</span> <span class="n">plot_decision_boundaries</span><span class="p">(</span><span class="n">svm</span><span class="p">,</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/25d5cae6c9ee5141d5303997750fa7c786bbcaee54241d22e6ac35df01fc1c9b.png" src="../../../../_images/25d5cae6c9ee5141d5303997750fa7c786bbcaee54241d22e6ac35df01fc1c9b.png" />
</div>
</div>
<p>We see the outer points in the dataset can be correctly classified, but we still struggle with the inner circle. But remember we have a circuit
with many free parameters! It is reasonable to believe we can give values to those variational parameters which improve the overall accuracy of our SVC.</p>
</section>
<section id="training-the-quantum-embedding-kernel">
<h2>Training the Quantum Embedding Kernel<a class="headerlink" href="#training-the-quantum-embedding-kernel" title="Permalink to this heading">¶</a></h2>
<p>To be able to train the Quantum Embedding Kernel we need some measure of how well it fits the dataset in question. Performing an exhaustive search in parameter space is not a good solution because it is very
resource intensive, and since the accuracy is a discrete quantity we would not be able to detect small improvements.</p>
<p>We can, however, resort to a more specialized measure, the <em>kernel-target alignment</em>. The kernel-target alignment compares the similarity predicted by the quantum kernel to the actual labels of the training data. It is based on <em>kernel alignment</em>, a similiarity measure between two kernels with given kernel matrices <span class="math notranslate nohighlight">\(K_1\)</span> and <span class="math notranslate nohighlight">\(K_2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\operatorname{KA}(K_1, K_2) = \frac{\operatorname{Tr}(K_1 K_2)}{\sqrt{\operatorname{Tr}(K_1^2)\operatorname{Tr}(K_2^2)}}.\]</div>
<p>Note</p>
<p>Seen from a more theoretical side, <span class="math notranslate nohighlight">\(\operatorname{KA}\)</span> is nothing else than the cosine of the angle between the kernel matrices <span class="math notranslate nohighlight">\(K_1\)</span> and <span class="math notranslate nohighlight">\(K_2\)</span>
if we see them as vectors in the space of matrices with the Hilbert-Schmidt (or Frobenius) scalar product <span class="math notranslate nohighlight">\(\langle A, B \rangle = \operatorname{Tr}(A^T B)\)</span>. This reinforces the
geometric picture of how this measure relates to objects, namely two kernels, being aligned in a vector space.</p>
<p>The training data enters the picture by defining an <em>ideal</em> kernel function that expresses the original labelling in the vector <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> by assigning to two datapoints the product of the
corresponding labels:</p>
<div class="math notranslate nohighlight">
\[k_{\boldsymbol{y}}(\boldsymbol{x}_i, \boldsymbol{x}_j) = y_i y_j.\]</div>
<p>The assigned kernel is thus <span class="math notranslate nohighlight">\(+1\)</span> if both datapoints lie in the same
class and <span class="math notranslate nohighlight">\(-1\)</span> otherwise and its kernel matrix is simply given by the
outer product <span class="math notranslate nohighlight">\(\boldsymbol{y}\boldsymbol{y}^T\)</span>. The kernel-target
alignment is then defined as the kernel alignment of the kernel matrix
<span class="math notranslate nohighlight">\(K\)</span> generated by the quantum kernel and
<span class="math notranslate nohighlight">\(\boldsymbol{y}\boldsymbol{y}^T\)</span>:</p>
<div class="math notranslate nohighlight">
\[\operatorname{KTA}_{\boldsymbol{y}}(K)
= \frac{\operatorname{Tr}(K \boldsymbol{y}\boldsymbol{y}^T)}{\sqrt{\operatorname{Tr}(K^2)\operatorname{Tr}((\boldsymbol{y}\boldsymbol{y}^T)^2)}}
= \frac{\boldsymbol{y}^T K \boldsymbol{y}}{\sqrt{\operatorname{Tr}(K^2)} N}\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the number of elements in <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>, that is the
number of datapoints in the dataset.</p>
<p>In summary, the kernel-target alignment effectively captures how well
the kernel you chose reproduces the actual similarities of the data. It
does have one drawback, however: having a high kernel-target alignment
is only a necessary but not a sufficient condition for a good
performance of the kernel. This means having good alignment is
guaranteed for good performance, but optimal alignment will not always
bring optimal training accuracy with it.</p>
<p>Let’s now come back to the actual implementation. PennyLane’s
<code class="docutils literal notranslate"><span class="pre">kernels</span></code> module allows you to easily evaluate the kernel target
alignment:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kta_init</span> <span class="o">=</span> <span class="n">qml</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">target_alignment</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">init_kernel</span><span class="p">,</span> <span class="n">assume_normalized_kernel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The kernel-target alignment for our dataset and random parameters is </span><span class="si">{</span><span class="n">kta_init</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The kernel-target alignment for our dataset and random parameters is 0.081
</pre></div>
</div>
</div>
</div>
<p>Now let’s code up an optimization loop and improve the kernel-target
alignment!</p>
<p>We will make use of regular gradient descent optimization. To speed up
the optimization we will not use the entire training set to compute
<span class="math notranslate nohighlight">\(\operatorname{KTA}\)</span> but rather sample smaller subsets of the data at
each step, we choose <span class="math notranslate nohighlight">\(4\)</span> datapoints at random. Remember that
PennyLane’s built-in optimizer works to <em>minimize</em> the cost function
that is given to it, which is why we have to multiply the kernel target
alignment by <span class="math notranslate nohighlight">\(-1\)</span> to actually <em>maximize</em> it in the process.</p>
<p>Note</p>
<p>Currently, the function <code class="docutils literal notranslate"><span class="pre">qml.kernels.target_alignment</span></code> is not
differentiable yet, making it unfit for gradient descent optimization.
We therefore first define a differentiable version of this function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">target_alignment</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">Y</span><span class="p">,</span>
    <span class="n">kernel</span><span class="p">,</span>
    <span class="n">assume_normalized_kernel</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">rescale_class_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Kernel-target alignment between kernel and labels.&quot;&quot;&quot;</span>

    <span class="n">K</span> <span class="o">=</span> <span class="n">qml</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">square_kernel_matrix</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span>
        <span class="n">kernel</span><span class="p">,</span>
        <span class="n">assume_normalized_kernel</span><span class="o">=</span><span class="n">assume_normalized_kernel</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">rescale_class_labels</span><span class="p">:</span>
        <span class="n">nplus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">nminus</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">-</span> <span class="n">nplus</span>
        <span class="n">_Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y</span> <span class="o">/</span> <span class="n">nplus</span> <span class="k">if</span> <span class="n">y</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">y</span> <span class="o">/</span> <span class="n">nminus</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">Y</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

    <span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">_Y</span><span class="p">,</span> <span class="n">_Y</span><span class="p">)</span>
    <span class="n">inner_product</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span> <span class="o">*</span> <span class="n">T</span><span class="p">)</span>
    <span class="n">norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span> <span class="o">*</span> <span class="n">K</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">T</span> <span class="o">*</span> <span class="n">T</span><span class="p">))</span>
    <span class="n">inner_product</span> <span class="o">=</span> <span class="n">inner_product</span> <span class="o">/</span> <span class="n">norm</span>

    <span class="k">return</span> <span class="n">inner_product</span>


<span class="n">params</span> <span class="o">=</span> <span class="n">init_params</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">qml</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="c1"># Choose subset of datapoints to compute the KTA on.</span>
    <span class="n">subset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))),</span> <span class="mi">4</span><span class="p">)</span>
    <span class="c1"># Define the cost function for optimization</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">_params</span><span class="p">:</span> <span class="o">-</span><span class="n">target_alignment</span><span class="p">(</span>
        <span class="n">X</span><span class="p">[</span><span class="n">subset</span><span class="p">],</span>
        <span class="n">Y</span><span class="p">[</span><span class="n">subset</span><span class="p">],</span>
        <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">:</span> <span class="n">kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">_params</span><span class="p">),</span>
        <span class="n">assume_normalized_kernel</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Optimization step</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="c1"># Report the alignment on the full dataset every 50 steps.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">current_alignment</span> <span class="o">=</span> <span class="n">target_alignment</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span>
            <span class="n">Y</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">:</span> <span class="n">kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">params</span><span class="p">),</span>
            <span class="n">assume_normalized_kernel</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Step </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> - Alignment = </span><span class="si">{</span><span class="n">current_alignment</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step 50 - Alignment = 0.098
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step 100 - Alignment = 0.121
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step 150 - Alignment = 0.141
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step 200 - Alignment = 0.173
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step 250 - Alignment = 0.196
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step 300 - Alignment = 0.224
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step 350 - Alignment = 0.245
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step 400 - Alignment = 0.261
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step 450 - Alignment = 0.276
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Step 500 - Alignment = 0.289
</pre></div>
</div>
</div>
</div>
<p>We want to assess the impact of training the parameters of the quantum
kernel. Thus, let’s build a second support vector classifier with the
trained kernel:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First create a kernel with the trained parameter baked into it.</span>
<span class="n">trained_kernel</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">:</span> <span class="n">kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

<span class="c1"># Second create a kernel matrix function using the trained kernel.</span>
<span class="n">trained_kernel_matrix</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">:</span> <span class="n">qml</span><span class="o">.</span><span class="n">kernels</span><span class="o">.</span><span class="n">kernel_matrix</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">trained_kernel</span><span class="p">)</span>

<span class="c1"># Note that SVC expects the kernel argument to be a kernel matrix function.</span>
<span class="n">svm_trained</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">trained_kernel_matrix</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We expect to see an accuracy improvement vs. the SVM with random
parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy_trained</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">svm_trained</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The accuracy of a kernel with trained parameters is </span><span class="si">{</span><span class="n">accuracy_trained</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The accuracy of a kernel with trained parameters is 1.000
</pre></div>
</div>
</div>
</div>
<p>We have now achieved perfect classification!</p>
<p>Following on the results that SVM’s have proven good generalisation
behavior, it will be interesting to inspect the decision boundaries of
our classifier:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trained_plot_data</span> <span class="o">=</span> <span class="n">plot_decision_boundaries</span><span class="p">(</span><span class="n">svm_trained</span><span class="p">,</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/96e73d80b09ecf56f04e71d503658d05e180cf388c892bee3f114f1a9e51d6f5.png" src="../../../../_images/96e73d80b09ecf56f04e71d503658d05e180cf388c892bee3f114f1a9e51d6f5.png" />
</div>
</div>
<p>Indeed, we see that now not only every data instance falls within the
correct class, but also that there are no strong artifacts that would
make us distrust the model. In this sense, our approach benefits from
both: on one hand it can adjust itself to the dataset, and on the other
hand is not expected to suffer from bad generalisation.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="how-to-approximate-a-classical-kernel-with-a-quantum-computer">
<h1>How to approximate a classical kernel with a quantum computer<a class="headerlink" href="#how-to-approximate-a-classical-kernel-with-a-quantum-computer" title="Permalink to this heading">¶</a></h1>
<p>Forget about advantages, supremacies, or speed-ups. Let us understand better what we can and cannot do with a quantum computer. More specifically, in this demo, we want to look into quantum kernels and ask whether we can replicate classical kernel functions with a quantum  computer. Lots of researchers have lengthily stared at the opposite question, namely that of classical simulation of quantum algorithms. Yet, by studying what classes of functions we can realize with quantum kernels, we can gain some insight into their inner workings.</p>
<p>Usually, in quantum machine learning (QML), we use parametrized quantum circuits (PQCs) to find good functions, whatever <em>good</em> means here. Since kernels are just one specific kind of well-defined functions, the task of finding a quantum kernel (QK) that approximates a given classical one could be posed as an optimization problem. One way to attack this task is to define a loss function quantifying the distance between both functions (the classical kernel function and the PQC-based hypothesis). This sort of approach does not help us much to gain theoretical insights about the structure of kernel-emulating quantum circuits, though.</p>
<p>In order to build intuition, we will instead study the link between classical and quantum kernels through the lens of the Fourier representation of a kernel, which is a common tool in classical machine learning. Two functions can only have the same Fourier spectrum if they are the same function. It turns out that, for certain classes of quantum circuits, <a class="reference external" href="https://pennylane.ai/qml/demos/tutorial_expressivity_fourier_series.html">we can theoretically describe the Fourier spectrum rather well</a>.</p>
<p>Using this theory, together with some good old-fashioned convex optimization, we will derive a quantum circuit that approximates the famous Gaussian kernel.</p>
<p>In order to keep the demo short and sweet, we focus on one simple example. The same ideas apply to more general scenarios. So tag along if you’d like to see how we build
a quantum kernel that approximates the well-known Gaussian kernel function!</p>
<p>|</p>
<p><img alt="Schematic of the steps covered in thisdemo." src="../../../../_images/classical_kernels_flowchart.PNG" /></p>
<section id="kernel-based-machine-learning">
<h2>Kernel-based Machine Learning<a class="headerlink" href="#kernel-based-machine-learning" title="Permalink to this heading">¶</a></h2>
<p>For the purposes of this demo, a <em>kernel</em> is a real-valued function of two variables <span class="math notranslate nohighlight">\(k(x_1,x_2)\)</span> from a given data domain <span class="math notranslate nohighlight">\(x_1, x_2\in\mathcal{X}\)</span>. In this demo, we’ll deal with real vector spaces as the data domain <span class="math notranslate nohighlight">\(\mathcal{X}\subseteq\mathbb{R}^d\)</span>, of some dimension <span class="math notranslate nohighlight">\(d\)</span>. A kernel has to be symmetric with respect to exchanging both variables <span class="math notranslate nohighlight">\(k(x_1,x_2) = k(x_2,x_1)\)</span>. We also enforce kernels to be positive semi-definite, but let’s avoid getting lost in mathematical lingo. You can trust that all kernels featured in this demo are positive semi-definite.</p>
</section>
<section id="shift-invariant-kernels">
<h2>Shift-invariant kernels<a class="headerlink" href="#shift-invariant-kernels" title="Permalink to this heading">¶</a></h2>
<p>Some kernels fulfill another important restriction, called <em>shift-invariance</em>. Shift-invariant kernels are those whose value doesn’t change if we add a shift to both inputs. Explicitly, for any suitable shift vector <span class="math notranslate nohighlight">\(\zeta\in\mathcal{X}\)</span>, shift-invariant kernels are those for which <span class="math notranslate nohighlight">\(k(x_1+\zeta,x_2+\zeta)=k(x_1,x_2)\)</span> holds. Having this property means the function can be written in terms of only one variable, which we call the <em>lag vector</em> <span class="math notranslate nohighlight">\(\delta:=x_1-x_2\in\mathcal{X}\)</span>. Abusing notation a bit:</p>
<div class="math notranslate nohighlight">
\[k(x_1,x_2)=k(x_1-x_2,0) = k(\delta).\]</div>
<p>For shift-invariant kernels, the exchange symmetry property <span class="math notranslate nohighlight">\(k(x_1,x_2)=k(x_2,x_1)\)</span> translates into reflection symmetry<span class="math notranslate nohighlight">\(k(\delta)=k(-\delta)\)</span>. Accordingly, we say <span class="math notranslate nohighlight">\(k\)</span> is an <em>even function</em>.</p>
</section>
<section id="warm-up-implementing-the-gaussian-kernel">
<h2>Warm up: Implementing the Gaussian kernel<a class="headerlink" href="#warm-up-implementing-the-gaussian-kernel" title="Permalink to this heading">¶</a></h2>
<p>First, let’s introduce a simple classical kernel that we will approximate on the quantum computer. Start importing the usual suspects:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pennylane</span> <span class="k">as</span> <span class="nn">qml</span>
<span class="kn">from</span> <span class="nn">pennylane</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">53173</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We’ll look at the Gaussian kernel:
<span class="math notranslate nohighlight">\(k_\sigma(x_1,x_2):=e^{-\lVert x_1-x_2\rVert^2/2\sigma^2}\)</span>. This
function is clearly shift-invariant:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
k_\sigma(x_1+\zeta,x_2+\zeta) &amp;= e^{-\lVert(x_1+\zeta)-(x_2+\zeta)\rVert^2/2\sigma^2} \\
&amp; = e^{-\lVert x_1-x_2\rVert^2/2\sigma^2} \\
&amp; = k_\sigma(x_1,x_2).
\end{aligned}\end{split}\]</div>
<p>The object of our study will be a simple version of the Gaussian kernel, where we consider <span class="math notranslate nohighlight">\(1\)</span>-dimensional data, so <span class="math notranslate nohighlight">\(\lVert x_1-x_2\rVert^2=(x_1-x_2)^2\)</span>. Also, we take <span class="math notranslate nohighlight">\(\sigma=1/\sqrt{2}\)</span> so that we further simplify the exponent. We can always re-introduce it later by rescaling the data. Again, we can write the function in terms of the lag vector only:</p>
<div class="math notranslate nohighlight">
\[k(\delta)=e^{-\delta^2}.\]</div>
<p>Now let’s write a few lines to plot the Gaussian kernel:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gaussian_kernel</span><span class="p">(</span><span class="n">delta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">delta</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">make_data</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">higher</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="n">higher</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">gaussian_kernel</span><span class="p">(</span><span class="n">x_</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span>

<span class="n">X</span><span class="p">,</span> <span class="n">Y_gaussian</span> <span class="o">=</span> <span class="n">make_data</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y_gaussian</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;The Gaussian kernel with $\sigma=1/\sqrt</span><span class="si">{2}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$\delta$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$k(\delta)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/8e7654dd49cbc7fa64e6559b878b9d04d35d6306dbdc10050aa79e1f73d64f5b.png" src="../../../../_images/8e7654dd49cbc7fa64e6559b878b9d04d35d6306dbdc10050aa79e1f73d64f5b.png" />
</div>
</div>
<p>In this demo, we will consider only this one example. However, the
arguments we make and the code we use are also amenable to any kernel
with the following mild restrictions:</p>
<ol class="arabic simple">
<li><p>Shift-invariance</p></li>
<li><p>Normalization <span class="math notranslate nohighlight">\(k(0)=1\)</span>.</p></li>
<li><p>Smoothness (in the sense of a quickly decaying Fourier spectrum).</p></li>
</ol>
<p>Note that is a very large class of kernels! And also an important one
for practical applications.</p>
</section>
<section id="fourier-analysis-of-the-gaussian-kernel">
<h2>Fourier analysis of the Gaussian kernel<a class="headerlink" href="#fourier-analysis-of-the-gaussian-kernel" title="Permalink to this heading">¶</a></h2>
<p>The next step will be to find the Fourier spectrum of the Gaussian kernel, which is an easy problem for classical computers. Once we’ve found it, we’ll build a QK that produces a finite Fourier series approximation to that spectrum.</p>
<p>Let’s briefly recall that a Fourier series is the representation of a periodic function using the sine and cosine functions. Fourier analysis tells us that we can write any given periodic function as</p>
<div class="math notranslate nohighlight">
\[f(x) = a_0 + \sum_{n=1}^\infty a_n\cos(n\omega_0x) + b_n\sin(n\omega_0x).\]</div>
<p>For that, we only need to find the suitable base frequency <span class="math notranslate nohighlight">\(\omega_0\)</span> and coefficients <span class="math notranslate nohighlight">\(a_0, a_1, \ldots, b_0, b_1,\ldots\)</span>.</p>
<p>But the Gaussian kernel is an aperiodic function, whereas the Fourier series only makes sense for periodic functions!</p>
<p><em>What can we do?!</em></p>
<p>We can cook up a periodic extension to the Gaussian kernel, for a given period <span class="math notranslate nohighlight">\(2L\)</span> (we take <span class="math notranslate nohighlight">\(L=\pi\)</span> as default):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">Gauss_p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">L</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">):</span>
    <span class="c1"># Send x to x_mod in the period around 0</span>
    <span class="n">x_mod</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mod</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">L</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">L</span><span class="p">)</span> <span class="o">-</span> <span class="n">L</span>
    <span class="k">return</span> <span class="n">gaussian_kernel</span><span class="p">(</span><span class="n">x_mod</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>which we can now plot</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_func</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">321</span><span class="p">)</span>
<span class="n">y_func</span> <span class="o">=</span> <span class="p">[</span><span class="n">Gauss_p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x_func</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_func</span><span class="p">,</span> <span class="n">y_func</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$\delta$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Periodic extension to the Gaussian kernel&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/0d9ba285847b1b0d30a2cdcc82a17de3b4f831fd21c110bfc295a1326eeb4dfc.png" src="../../../../_images/0d9ba285847b1b0d30a2cdcc82a17de3b4f831fd21c110bfc295a1326eeb4dfc.png" />
</div>
</div>
<p>In practice, we would construct several periodic extensions of the aperiodic function, with increasing periods. This way, we can study the behaviour when the period approaches infinity, i.e. the regime where the function stops being periodic.</p>
<p>Next up, how does the Fourier spectrum of such an object look like? We can find out using PennyLane’s <code class="docutils literal notranslate"><span class="pre">fourier</span></code> module!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pennylane.fourier</span> <span class="kn">import</span> <span class="n">coefficients</span>
</pre></div>
</div>
</div>
</div>
<p>The function <code class="docutils literal notranslate"><span class="pre">coefficients</span></code> computes for us the coefficients of the Fourier series up to a fixed term. One tiny detail here: <code class="docutils literal notranslate"><span class="pre">coefficients</span></code> returns one complex number <span class="math notranslate nohighlight">\(c_n\)</span> for each frequency <span class="math notranslate nohighlight">\(n\)</span>. The real part corresponds to the <span class="math notranslate nohighlight">\(a_n\)</span> coefficient, and the imaginary part to the <span class="math notranslate nohighlight">\(b_n\)</span> coefficient: <span class="math notranslate nohighlight">\(c_n=a_n+ib_n\)</span>. Because the Gaussian kernel is an even function, we know that the imaginary part of every coefficient will be zero, so <span class="math notranslate nohighlight">\(c_n=a_n\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fourier_p</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    We only take the first d coefficients [:d]</span>
<span class="sd">    because coefficients() treats the negative frequencies</span>
<span class="sd">    as different from the positive ones.</span>
<span class="sd">    For real functions, they are the same.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">coefficients</span><span class="p">(</span><span class="n">Gauss_p</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)[:</span><span class="n">d</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We are restricted to considering only a finite number of Fourier terms. But isn’t that problematic, one may say? Well, maybe. Since we know the Gaussian kernel is a smooth function, we expect that the coefficients
converge to <span class="math notranslate nohighlight">\(0\)</span> at some point, and we will only need to consider terms up to this point. Let’s look at the coefficients we obtain by setting a low value for the number of coefficients and then slowly letting it grow:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">7</span><span class="p">):</span>
    <span class="n">N</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">F</span> <span class="o">=</span> <span class="n">fourier_p</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;frequency $n$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Fourier coefficient $c_n$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/f6162fab31c4051e5706b1238ba71744a1857f13b6822088aa44985243331423.png" src="../../../../_images/f6162fab31c4051e5706b1238ba71744a1857f13b6822088aa44985243331423.png" />
</div>
</div>
<p>What do we see? For very small coefficient counts, like <span class="math notranslate nohighlight">\(2\)</span> and <span class="math notranslate nohighlight">\(3\)</span>, we see that the last allowed coefficient is still far from <span class="math notranslate nohighlight">\(0\)</span>. That’s a very clear indicator that we need to consider more frequencies. At the same time, it seems like starting at <span class="math notranslate nohighlight">\(5\)</span> or <span class="math notranslate nohighlight">\(6\)</span> all the non-zero contributions have already been well captured. This is important for us, since it tells us the minimum number of qubits we should use. One can see that every new qubit doubles the number of frequencies we can use, so for <span class="math notranslate nohighlight">\(n\)</span> qubits, we will have <span class="math notranslate nohighlight">\(2^n\)</span>. At minimum of <span class="math notranslate nohighlight">\(6\)</span> frequencies means at least <span class="math notranslate nohighlight">\(3\)</span> qubits, corresponding to <span class="math notranslate nohighlight">\(2^3=8\)</span> frequencies. As we’ll see later, we’ll work with <span class="math notranslate nohighlight">\(5\)</span> qubits, so <span class="math notranslate nohighlight">\(32\)</span> frequencies. That means the spectrum we will be trying to replicate will be the following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span> <span class="n">fourier_p</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span> <span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;frequency $n$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Fourier coefficient $c_n$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Fourier spectrum of the Gaussian kernel&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/60f05fda5a779714136dfc43bb0a322571cf185a3c69b3135598066e3fdde5b1.png" src="../../../../_images/60f05fda5a779714136dfc43bb0a322571cf185a3c69b3135598066e3fdde5b1.png" />
</div>
</div>
<p>We just need a QK with the same Fourier spectrum!</p>
</section>
<section id="designing-a-suitable-qk">
<h2>Designing a suitable QK<a class="headerlink" href="#designing-a-suitable-qk" title="Permalink to this heading">¶</a></h2>
<p>Designing a suitable QK amounts to designing a suitable parametrized quantum circuit. Let’s take a moment to refresh the big scheme of things with the following picture:</p>
<p><img alt="The quantum kernel considered in this demo." src="../../../../_images/QEK.jpg" /></p>
<p>We construct the quantum kernel from a quantum embedding. The quantum embedding circuit will consist of two parts. The first one, trainable, will be a parametrized general state preparation scheme <span class="math notranslate nohighlight">\(W_a\)</span>, with parameters <span class="math notranslate nohighlight">\(a\)</span>. In the second one, we input the data, denoted by <span class="math notranslate nohighlight">\(S(x)\)</span>.</p>
<p>Start with the non-trainable gate we’ll use to encode the data <span class="math notranslate nohighlight">\(S(x)\)</span>. It consists of applying one Pauli-<span class="math notranslate nohighlight">\(Z\)</span> rotation to each qubit with rotation parameter <span class="math notranslate nohighlight">\(x\)</span> times some constant <span class="math notranslate nohighlight">\(\vartheta_i\)</span>, for the <span class="math notranslate nohighlight">\(i^\text{th}\)</span> qubit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">S</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">wires</span><span class="p">):</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">wire</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">wires</span><span class="p">):</span>
        <span class="n">qml</span><span class="o">.</span><span class="n">RZ</span><span class="p">(</span><span class="n">thetas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="n">wires</span> <span class="o">=</span> <span class="p">[</span><span class="n">wire</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>By setting the <code class="docutils literal notranslate"><span class="pre">thetas</span></code> properly, we achieve the integer-valued spectrum, as required by the Fourier series expansion of a function of period <span class="math notranslate nohighlight">\(2\pi\)</span>: <span class="math notranslate nohighlight">\(\{0, 1, \ldots, 2^n-2, 2^n-1\}\)</span>, for <span class="math notranslate nohighlight">\(n\)</span> qubits. Some math shows that setting <span class="math notranslate nohighlight">\(\vartheta_i=2^{n-i}\)</span>, for <span class="math notranslate nohighlight">\(\{1,\ldots,n\}\)</span> produces the desired outcome.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_thetas</span><span class="p">(</span><span class="n">n_wires</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="mi">2</span> <span class="o">**</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_wires</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we introduce the only trainable gate we need to make use of. Contrary to the usual Ansätze used in supervised and unsupervised learning, we use a state preparation template called <code class="docutils literal notranslate"><span class="pre">MottonenStatePreparation</span></code>. This is one option for amplitude encoding already implemented in PennyLane, so we don’t need to code it ourselves. Amplitude encoding is a common way of embedding classical data into a quantum system in QML. The unitary associated to this
template transforms the <span class="math notranslate nohighlight">\(\lvert0\rangle\)</span> state into a state with amplitudes <span class="math notranslate nohighlight">\(a=(a_0,a_1,\ldots,a_{2^n-1})\)</span>, namely <span class="math notranslate nohighlight">\(\lvert a\rangle=\sum_j a_j\lvert j\rangle\)</span>, provided <span class="math notranslate nohighlight">\(\lVert a\rVert^2=1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">W</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">wires</span><span class="p">):</span>
    <span class="n">qml</span><span class="o">.</span><span class="n">templates</span><span class="o">.</span><span class="n">state_preparations</span><span class="o">.</span><span class="n">MottonenStatePreparation</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">wires</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>With that, we have the feature map onto the Hilbert space of the quantum computer:</p>
<div class="math notranslate nohighlight">
\[\lvert x_a\rangle = S(x)W_a\lvert0\rangle,\]</div>
<p>for a given <span class="math notranslate nohighlight">\(a\)</span>, which we will specify later.</p>
<p>Accordingly, we can build the QK corresponding to this feature map as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
k_a(x_1,x_2) &amp;= \lvert\langle0\rvert W_a^\dagger S^\dagger(x_1)
S(x_2)W_a\lvert0\rangle\rvert^2 \\
&amp;= \lvert\langle0\rvert W_a^\dagger S(x_2-x_1) W_a\lvert0\rangle\rvert^2.
\end{aligned}\end{split}\]</div>
<p>In the code below, the variable <code class="docutils literal notranslate"><span class="pre">amplitudes</span></code> corresponds to our set <span class="math notranslate nohighlight">\(a\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ansatz</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">amplitudes</span><span class="p">,</span> <span class="n">wires</span><span class="p">):</span>
    <span class="n">W</span><span class="p">(</span><span class="n">amplitudes</span><span class="p">,</span> <span class="n">wires</span><span class="p">)</span>
    <span class="n">S</span><span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">wires</span><span class="p">)</span>
    <span class="n">qml</span><span class="o">.</span><span class="n">adjoint</span><span class="p">(</span><span class="n">W</span><span class="p">)(</span><span class="n">amplitudes</span><span class="p">,</span> <span class="n">wires</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Since this kernel is by construction real-valued, we also have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
(k_a(x_1,x_2))^\ast &amp;= k_a(x_1,x_2) \\
&amp;= \lvert\langle0\rvert W_a^\dagger S(x_1-x_2) W_a\lvert0\rangle\rvert^2 \\
&amp;= k_a(x_2,x_1).
\end{aligned}\end{split}\]</div>
<p>Further, this QK is also shift-invariant <span class="math notranslate nohighlight">\(k_a(x_1,x_2) = k_a(x_1+\zeta,
x_2+\zeta)\)</span> for any <span class="math notranslate nohighlight">\(\zeta\in\mathbb{R}\)</span>. So we can also write it in
terms of the lag <span class="math notranslate nohighlight">\(\delta=x_1-x_2\)</span>:</p>
<div class="math notranslate nohighlight">
\[k_a(\delta) = \lvert\langle0\rvert W_a^\dagger
S(\delta)W_a\lvert0\rangle\rvert^2.\]</div>
<p>So far, we only wrote the gate layout for the quantum circuit, no measurement! We need a few more functions for that!</p>
</section>
<section id="computing-the-qk-function-on-a-quantum-device">
<h2>Computing the QK function on a quantum device<a class="headerlink" href="#computing-the-qk-function-on-a-quantum-device" title="Permalink to this heading">¶</a></h2>
<p>Also, at this point, we need to set the number of qubits of our computer. For this example, we’ll use the variable <code class="docutils literal notranslate"><span class="pre">n_wires</span></code>, and set it to <span class="math notranslate nohighlight">\(5\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_wires</span> <span class="o">=</span> <span class="mi">5</span>
</pre></div>
</div>
</div>
</div>
<p>We initialize the quantum simulator:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dev</span> <span class="o">=</span> <span class="n">qml</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;default.qubit&quot;</span><span class="p">,</span> <span class="n">wires</span> <span class="o">=</span> <span class="n">n_wires</span><span class="p">,</span> <span class="n">shots</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we construct the quantum node:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@qml</span><span class="o">.</span><span class="n">qnode</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">QK_circuit</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">amplitudes</span><span class="p">):</span>
    <span class="n">ansatz</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">amplitudes</span><span class="p">,</span> <span class="n">wires</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_wires</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">qml</span><span class="o">.</span><span class="n">probs</span><span class="p">(</span><span class="n">wires</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_wires</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Recall that the output of a QK is defined as the probability of
obtaining the outcome <span class="math notranslate nohighlight">\(\lvert0\rangle\)</span> when measuring in the
computational basis. That corresponds to the <span class="math notranslate nohighlight">\(0^\text{th}\)</span> entry of
<code class="docutils literal notranslate"><span class="pre">qml.probs</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">QK_2</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">amplitudes</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">QK_circuit</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">amplitudes</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>As a couple of quality-of-life improvements, we write a function that
implements the QK with the lag <span class="math notranslate nohighlight">\(\delta\)</span> as its argument, and one that
implements it on a given set of data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">QK</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">amplitudes</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">QK_2</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">amplitudes</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">QK_on_dataset</span><span class="p">(</span><span class="n">deltas</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">amplitudes</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">QK</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">amplitudes</span><span class="p">)</span> <span class="k">for</span> <span class="n">delta</span> <span class="ow">in</span> <span class="n">deltas</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<p>This is also a good place to fix the <code class="docutils literal notranslate"><span class="pre">thetas</span></code> array, so that we don’t
forget later.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thetas</span> <span class="o">=</span> <span class="n">make_thetas</span><span class="p">(</span><span class="n">n_wires</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s see how this looks like for one particular choice of
<code class="docutils literal notranslate"><span class="pre">amplitudes</span></code>. We need to make sure the array fulfills the normalization
conditions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">n_wires</span><span class="p">)])</span>
<span class="n">test_amplitudes</span> <span class="o">=</span> <span class="n">test_features</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">test_features</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">Y_test</span> <span class="o">=</span> <span class="n">QK_on_dataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">test_amplitudes</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$\delta$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;QK with test amplitudes&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/56a69f97d44fa7abfe502b65ceab9c39d2bfb8536e44bf91e61d2ebdf9ebf34b.png" src="../../../../_images/56a69f97d44fa7abfe502b65ceab9c39d2bfb8536e44bf91e61d2ebdf9ebf34b.png" />
</div>
</div>
<p>One can see that the stationary kernel with this particular initial state has a decaying spectrum that looks similar to <span class="math notranslate nohighlight">\(1/\lvert x\rvert\)</span> — but not yet like a Gaussian.</p>
</section>
<section id="how-to-find-the-amplitudes-emulating-a-gaussian-kernel">
<h2>How to find the amplitudes emulating a Gaussian kernel<a class="headerlink" href="#how-to-find-the-amplitudes-emulating-a-gaussian-kernel" title="Permalink to this heading">¶</a></h2>
<p>If we knew exactly which amplitudes to choose in order to build a given Fourier spectrum, our job would be done here. However, the equations derived in the literature are not trivial to solve.</p>
<p>As mentioned in the introduction, one could just “learn” this relation, that is, tune the parameters of the quantum kernel in a gradient-based manner until it matches the classical one.</p>
<p>We want to take an intermediate route between analytical solution and black-box optimization. For that, we derive an equation that links the amplitudes to the spectrum we want to construct and then use
old-fashioned convex optimization to find the solution. If you are not interested in the details, you can just jump to the last plots of this demo and confirm that we can to emulate the Gaussian kernel using the ansatz for our QK constructed above.</p>
<p>In order to simplify the formulas, we introduce new variables, which we call <code class="docutils literal notranslate"><span class="pre">probabilities</span></code> <span class="math notranslate nohighlight">\((p_0, p_1, p_2, \ldots, p_{2^n-1})\)</span>, and we define as <span class="math notranslate nohighlight">\(p_j=\lvert a_j\rvert^2\)</span>. Following the normalization property above, we have <span class="math notranslate nohighlight">\(\sum_j p_j=1\)</span>. Don’t get too fond of them, we only need them for this step! Remember we introduced the vector <span class="math notranslate nohighlight">\(a\)</span> for the <code class="docutils literal notranslate"><span class="pre">MottonenStatePreparation</span></code> as the <em>amplitudes</em> of a quantum state? Then it makes sense that we call its squares <em>probabilities</em>, doesn’t it?</p>
<p>There is a crazy formula that matches the entries of <em>probabilities</em> with the Fourier series of the resulting QK function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\text{probabilities} &amp;\longrightarrow \text{Fourier coefficients} \\
\begin{pmatrix} p_0 \\ p_1 \\ p_2 \\ \vdots \\ p_{2^n-1} \end{pmatrix}
&amp;\longmapsto \begin{pmatrix} \sum_{j=0}^{2^n-1} p_j^2 \\ \sum_{j=1}^{2^n-1}
p_j p_{j-1} \\ \sum_{j=2}^{2^n-1} p_j p_{j-2} \\ \vdots \\ p_{2^n-1} p_0
\end{pmatrix}
\end{aligned}\end{split}\]</div>
<p>This looks a bit scary, it follows from expanding the matrix product <span class="math notranslate nohighlight">\(W_a^\dagger S(\delta)W_a\)</span>, and then collecting terms according to Fourier basis monomials. In this sense, the formula is general and it applies to any shift-invariant kernel we might want to approximate, not only the Gaussian kernel.</p>
<p>Our goal is to find the set of <span class="math notranslate nohighlight">\(p_j\)</span>’s that produces the Fourier coefficients of a given kernel function (in our case, the Gaussian kernel), namely its spectrum <span class="math notranslate nohighlight">\((s_0, s_1, s_2, \ldots, s_{2^n-1})\)</span>. We consider now a slightly different map <span class="math notranslate nohighlight">\(F_s\)</span>, for a given spectrum <span class="math notranslate nohighlight">\((s_0, s_1, \ldots, s_{2^n-1})\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
F_s: \text{probabilities} &amp;\longrightarrow \text{Difference between Fourier
coefficients} \\
\begin{pmatrix} p_0 \\ p_1 \\ p_2 \\ \vdots \\ p_{2^n-1} \end{pmatrix}
&amp;\longmapsto \begin{pmatrix} \sum_{j=0}^{2^n-1} p_j^2 - s_0 \\
\sum_{j=1}^{2^n-1} p_j p_{j-1} - s_1 \\ \sum_{j=2}^{2^n-1} p_j
p_{j-2} - s_2 \\ \vdots \\ p_{2^n-1}p_0 - s_{2^n-1} \end{pmatrix}.
\end{aligned}\end{split}\]</div>
<p>If you look at it again, you’ll see that the zero (or solution) of this second map <span class="math notranslate nohighlight">\(F_s\)</span> is precisely the array of <em>probabilities</em> we are looking for. We can write down the first map as:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict_spectrum</span><span class="p">(</span><span class="n">probabilities</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>
    <span class="n">spectrum</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
        <span class="n">s_</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
            <span class="n">s_</span> <span class="o">+=</span> <span class="n">probabilities</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">probabilities</span><span class="p">[</span><span class="n">j</span> <span class="o">-</span> <span class="n">s</span><span class="p">]</span>

        <span class="n">spectrum</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>

    <span class="c1"># This is to make the output have the same format as</span>
    <span class="c1"># the output of pennylane.fourier.coefficients</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">d</span><span class="p">):</span>
        <span class="n">spectrum</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">spectrum</span><span class="p">[</span><span class="n">d</span> <span class="o">-</span> <span class="n">s</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">spectrum</span>
</pre></div>
</div>
</div>
</div>
<p>And then <span class="math notranslate nohighlight">\(F_s\)</span> is just <code class="docutils literal notranslate"><span class="pre">predict_spectrum</span></code> minus the spectrum we want to predict:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">F</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">spectrum</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">predict_spectrum</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)[:</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="n">spectrum</span><span class="p">[:</span><span class="n">d</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>These closed-form equations allow us to find the solution numerically, using Newton’s method! Newton’s method is a classical one from convex optimization theory. For our case, since the formula is quadratic, we rest assured that we are within the realm of convex functions.</p>
</section>
<section id="finding-the-solution">
<h2>Finding the solution<a class="headerlink" href="#finding-the-solution" title="Permalink to this heading">¶</a></h2>
<p>In order to use Newton’s method we need the Jacobian of <span class="math notranslate nohighlight">\(F_s\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">J_F</span><span class="p">(</span><span class="n">probabilities</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>
    <span class="n">J</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">):</span>
                <span class="n">J</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">probabilities</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span>
            <span class="k">if</span><span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">j</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
                <span class="n">J</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">probabilities</span><span class="p">[</span><span class="n">j</span> <span class="o">-</span> <span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">J</span>
</pre></div>
</div>
</div>
</div>
<p>Showing that this is indeed <span class="math notranslate nohighlight">\(\nabla F_s\)</span> is left as an exercise for the reader. For Newton’s method, we also need an initial guess. Finding a
good initial guess requires some tinkering; different problems will benefit from different ones. Here is a tame one that works for the Gaussian kernel.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_initial_probabilities</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="n">probabilities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">deg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">d</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">probabilities</span> <span class="o">=</span> <span class="n">probabilities</span> <span class="o">/</span> <span class="n">deg</span>
    <span class="k">return</span> <span class="n">probabilities</span>

<span class="n">probabilities</span> <span class="o">=</span> <span class="n">make_initial_probabilities</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">n_wires</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Recall the <code class="docutils literal notranslate"><span class="pre">spectrum</span></code> we want to match is that of the periodic extension of the Gaussian kernel.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spectrum</span> <span class="o">=</span> <span class="n">fourier_p</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">n_wires</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We fix the hyperparameters for Newton’s method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">n_wires</span>
<span class="n">max_steps</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">tol</span> <span class="o">=</span> <span class="mf">1.e-20</span>
</pre></div>
</div>
</div>
</div>
<p>And we’re good to go!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
    <span class="n">inc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">J_F</span><span class="p">(</span><span class="n">probabilities</span><span class="p">),</span> <span class="o">-</span><span class="n">F</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">spectrum</span><span class="p">))</span>
    <span class="n">probabilities</span> <span class="o">=</span> <span class="n">probabilities</span> <span class="o">+</span> <span class="n">inc</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error norm at step </span><span class="si">{0:3}</span><span class="s2">: </span><span class="si">{1}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                                               <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span>
                                                                <span class="n">spectrum</span><span class="p">))))</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">spectrum</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tolerance trespassed! This is the end.&quot;</span><span class="p">)</span>
            <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Error norm at step  10: 7.231526379770506e-14
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Error norm at step  20: 5.551115536294438e-17
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Error norm at step  30: 9.2815769223824e-21
Tolerance trespassed! This is the end.
</pre></div>
</div>
</div>
</div>
<p>The tolerance we set was fairly low, one should expect good things to
come out of this. Let’s have a look at the solution:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">),</span> <span class="n">probabilities</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;array entry $j$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;probabilities $p_j$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/7db83e18e192ada151875a7f54e8c54a1bc938bff5520fd539b3692279f67a83.png" src="../../../../_images/7db83e18e192ada151875a7f54e8c54a1bc938bff5520fd539b3692279f67a83.png" />
</div>
</div>
<p>Would you be able to tell whether this is correct? Me neither! But all those probabilities being close to <span class="math notranslate nohighlight">\(0\)</span> should make us fear some of them must’ve turned negative. That would be fatal for us. For <code class="docutils literal notranslate"><span class="pre">MottonenStatePreparation</span></code>, we’ll need to give <code class="docutils literal notranslate"><span class="pre">amplitudes</span></code> as one of the arguments, which is the component-wise square root of <code class="docutils literal notranslate"><span class="pre">probabilities</span></code>. And hence the problem! Even if they are very small values, if any entry of <code class="docutils literal notranslate"><span class="pre">probabilities</span></code> is negative, the square root will give <code class="docutils literal notranslate"><span class="pre">nan</span></code>. In order to avoid that, we use a simple thresholding where we replace very small entries by <span class="math notranslate nohighlight">\(0\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">probabilities_threshold_normalize</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">thresh</span> <span class="o">=</span> <span class="mf">1.e-10</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>
    <span class="n">p_t</span> <span class="o">=</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">probabilities</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">thresh</span><span class="p">):</span>
            <span class="n">p_t</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="n">p_t</span> <span class="o">=</span> <span class="n">p_t</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p_t</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">p_t</span>
</pre></div>
</div>
</div>
</div>
<p>Then, we need to take the square root:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probabilities</span> <span class="o">=</span> <span class="n">probabilities_threshold_normalize</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>
<span class="n">amplitudes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>A little plotting never killed nobody</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">),</span> <span class="n">probabilities</span><span class="p">,</span> <span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;probability $p_j = |a_j|^2$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">),</span> <span class="n">amplitudes</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;amplitude $a_j$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;array entry $j$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/9ee00500bbb4ff348ca049740d8868bdce2bc8f411f17dbfc4eff4a88cd3d3ad.png" src="../../../../_images/9ee00500bbb4ff348ca049740d8868bdce2bc8f411f17dbfc4eff4a88cd3d3ad.png" />
</div>
</div>
</section>
<section id="visualizing-the-solution">
<h2>Visualizing the solution<a class="headerlink" href="#visualizing-the-solution" title="Permalink to this heading">¶</a></h2>
<p>And the moment of truth! Does the solution really match the spectrum? We try it first using <code class="docutils literal notranslate"><span class="pre">predict_spectrum</span></code> only</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">),</span> <span class="n">fourier_p</span><span class="p">(</span><span class="n">d</span><span class="p">)[:</span><span class="n">d</span><span class="p">],</span> <span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Gaussian kernel&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">),</span> <span class="n">predict_spectrum</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)[:</span><span class="n">d</span><span class="p">],</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;QK predicted&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;frequency $n$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Fourier coefficient&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Fourier spectrum of the Gaussian kernel&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/37fc1c90f6ef60bfa2725762582d90bca99a72f0dcbb5b3e242d50ebe694ff90.png" src="../../../../_images/37fc1c90f6ef60bfa2725762582d90bca99a72f0dcbb5b3e242d50ebe694ff90.png" />
</div>
</div>
<p>It seems like it does! But as we just said, this is still only the predicted spectrum. We haven’t called the quantum computer at all yet!</p>
<p>Let’s see what happens when we call the function <code class="docutils literal notranslate"><span class="pre">coefficients</span></code> on the QK function we defined earlier. Good coding practice tells us we should probably turn this step into a function itself, in case it is of use later:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fourier_q</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">amplitudes</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">coefficients</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">QK</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">amplitudes</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>And with this, we can finally visualize how the Fourier spectrum of the QK function compares to that of the Gaussian kernel:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">),</span> <span class="n">fourier_p</span><span class="p">(</span><span class="n">d</span><span class="p">)[:</span><span class="n">d</span><span class="p">],</span> <span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Gaussian kernel&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">),</span> <span class="n">predict_spectrum</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)[:</span><span class="n">d</span><span class="p">],</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;QK predicted&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">),</span> <span class="n">fourier_q</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">amplitudes</span><span class="p">)[:</span><span class="n">d</span><span class="p">],</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;QK computer&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;frequency $n$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Fourier coefficient&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Fourier spectrum of the Gaussian kernel&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/fbc2e6774aa7bc053908a0cfc21681a9680cf448d93dbd71862d8f5b48161dea.png" src="../../../../_images/fbc2e6774aa7bc053908a0cfc21681a9680cf448d93dbd71862d8f5b48161dea.png" />
</div>
</div>
<p>It seems it went well! Matching spectra should mean matching kernel functions, right?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y_learned</span> <span class="o">=</span> <span class="n">QK_on_dataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">amplitudes</span><span class="p">)</span>
<span class="n">Y_truth</span> <span class="o">=</span> <span class="p">[</span><span class="n">Gauss_p</span><span class="p">(</span><span class="n">x_</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y_learned</span><span class="p">,</span> <span class="s1">&#39;-.&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;QK&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y_truth</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Gaussian kernel&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$\delta$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$k(\delta)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/44a027deacdb9d29e117d0d49ea7aee7ff78a7a2c8e40422dd9e26fa2cbce01d.png" src="../../../../_images/44a027deacdb9d29e117d0d49ea7aee7ff78a7a2c8e40422dd9e26fa2cbce01d.png" />
</div>
</div>
<p>Yeah! We did it!</p>
<p><img alt="" src="../../../../_images/salesman.PNG" /></p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="kernel-based-training-of-quantum-models-with-scikit-learn">
<h1>Kernel-based training of quantum models with scikit-learn<a class="headerlink" href="#kernel-based-training-of-quantum-models-with-scikit-learn" title="Permalink to this heading">¶</a></h1>
<p>Over the last few years, quantum machine learning research has provided a lot of insights on how we can understand and train quantum circuits as machine learning models. While many connections to neural networks have been made, it becomes increasingly clear that their mathematical foundation is intimately related to so-called <em>kernel methods</em>, the most famous of which is the <a class="reference external" href="https://en.wikipedia.org/wiki/Support-vector_machine">support vector machine (SVM)</a> (see for example <a class="reference external" href="https://arxiv.org/abs/1803.07128">Schuld and Killoran (2018)</a>, <a class="reference external" href="https://arxiv.org/abs/1804.11326">Havlicek et al. (2018)</a>, <a class="reference external" href="https://arxiv.org/abs/2010.02174">Liu et al. (2020)</a>, <a class="reference external" href="https://arxiv.org/pdf/2011.01938">Huang et al. (2020)</a>, and, for a systematic summary which we will follow here, <a class="reference external" href="https://arxiv.org/abs/2101.11020">Schuld (2021)</a>).</p>
<p>The link between quantum models and kernel methods has important practical implications: we can replace the common <a class="reference external" href="https://pennylane.ai/qml/glossary/variational_circuit.html">variational approach</a> to quantum machine learning with a classical kernel method where the kernel—a small building block of the overall algorithm—is computed by a quantum device. In many situations there are guarantees that we get better or at least equally good results.</p>
<p>This demonstration explores how kernel-based training compares with <a class="reference external" href="https://pennylane.ai/qml/demos/tutorial_variational_classifier.html">variational training</a> in terms of the number of quantum circuits that have to be evaluated.
For this we train a quantum machine learning model with a kernel-based approach using a combination of PennyLane and the <a class="reference external" href="https://scikit-learn.org/">scikit-learn</a> machine learning library. We compare this strategy with a variational quantum circuit trained via stochastic gradient descent using <a class="reference external" href="https://pennylane.readthedocs.io/en/stable/introduction/interfaces/torch.html">PyTorch</a>.</p>
<p>We will see that in a typical small-scale example, kernel-based training requires only a fraction of the number of quantum circuit evaluations used by variational circuit training, while each evaluation runs a much shorter circuit. In general, the relative efficiency of kernel-based methods compared to variational circuits depends on the number of parameters used in the variational model.</p>
<p><img alt="" src="../../../../_images/scaling1.png" /></p>
<p>If the number of variational parameters remains small, e.g., there is a square-root-like scaling with the number of data samples (green line), variational circuits are almost as efficient as neural networks (blue line), and require much fewer circuit evaluations than the quadratic scaling of kernel methods (red line). However, with current hardware-compatible training strategies, kernel methods scale much better than variational circuits that require a number of parameters of the order of the training set size (orange line).</p>
<p>In conclusion, <strong>for quantum machine learning applications with many parameters, kernel-based training can be a great alternative to the variational approach to quantum machine learning</strong>.</p>
<p>After working through this demo, you will:</p>
<ul class="simple">
<li><p>be able to use a support vector machine with a quantum kernel
computed with PennyLane, and</p></li>
<li><p>be able to compare the scaling of quantum circuit evaluations
required in kernel-based versus variational training.</p></li>
</ul>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this heading">¶</a></h2>
<p>Let us consider a <em>quantum model</em> of the form</p>
<div class="math notranslate nohighlight">
\[f(x) = \langle \phi(x) | \mathcal{M} | \phi(x)\rangle,\]</div>
<p>where <span class="math notranslate nohighlight">\(| \phi(x)\rangle\)</span> is prepared by a fixed <a class="reference external" href="https://pennylane.ai/qml/glossary/quantum_embedding.html">embedding circuit</a> that encodes data inputs <span class="math notranslate nohighlight">\(x\)</span>, and <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> is an arbitrary observable.
This model includes variational quantum machine learning models, since the observable can effectively be implemented by a simple measurement that is preceded by a variational circuit:</p>
<p><img alt="" src="../../../../_images/quantum_model.png" /></p>
<p>For example, applying a circuit <span class="math notranslate nohighlight">\(G(\theta)\)</span> and then measuring the Pauli-Z observable <span class="math notranslate nohighlight">\(\sigma^0_z\)</span> of the first qubit implements the trainable measurement <span class="math notranslate nohighlight">\(\mathcal{M}(\theta) = G^{\dagger}(\theta) \sigma^0_z G(\theta)\)</span>.</p>
<p>The main practical consequence of approaching quantum machine learning with a kernel approach is that instead of training <span class="math notranslate nohighlight">\(f\)</span> variationally, we can often train an equivalent classical kernel method with a kernel
executed on a quantum device. This <em>quantum kernel</em> is given by the mutual overlap of two data-encoding quantum states,</p>
<div class="math notranslate nohighlight">
\[\kappa(x, x') = | \langle \phi(x') | \phi(x)\rangle|^2.\]</div>
<p>Kernel-based training therefore bypasses the processing and measurement parts of common variational circuits, and only depends on the data encoding.</p>
<p>If the loss function <span class="math notranslate nohighlight">\(L\)</span> is the <a class="reference external" href="https://en.wikipedia.org/wiki/Hinge_loss">hinge loss</a>, the kernel method corresponds to a standard <a class="reference external" href="https://en.wikipedia.org/wiki/Support-vector_machine">support vector machine</a> (SVM) in
the sense of a maximum-margin classifier. Other convex loss functions lead to more general variations of support vector machines.</p>
<p>Note:</p>
<p>More precisely, we can replace variational with kernel-based training if
the optimisation problem can be written as minimizing a cost of the form</p>
<div class="math notranslate nohighlight">
\[\min_f  \lambda\;  \mathrm{tr}\{\mathcal{M}^2\} + \frac{1}{M}\sum_{m=1}^M L(f(x^m), y^m),\]</div>
<p>which is a regularized empirical risk with training data samples
<span class="math notranslate nohighlight">\((x^m, y^m)_{m=1\dots M}\)</span>, regularization strength
<span class="math notranslate nohighlight">\(\lambda \in \mathbb{R}\)</span>, and loss function <span class="math notranslate nohighlight">\(L\)</span>.</p>
<p>Theory predicts that kernel-based training will always find better or
equally good minima of this risk. However, to show this here we would
have to either regularize the variational training by the trace of the
squared observable, or switch off regularization in the classical SVM,
which removes a lot of its strength. The kernel-based and the
variational training in this demonstration therefore optimize slightly
different cost functions, and it is out of our scope to establish
whether one training method finds a better minimum than the other.</p>
</section>
<section id="kernel-based-training">
<h2>Kernel-based training<a class="headerlink" href="#kernel-based-training" title="Permalink to this heading">¶</a></h2>
<p>First, we will turn to kernel-based training of quantum models. As stated above, an example implementation is a standard support vector machine with a kernel computed by a quantum circuit.</p>
<p>We begin by importing all sorts of useful methods:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="kn">import</span> <span class="n">relu</span>

<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="kn">import</span> <span class="nn">pennylane</span> <span class="k">as</span> <span class="nn">qml</span>
<span class="kn">from</span> <span class="nn">pennylane.templates</span> <span class="kn">import</span> <span class="n">AngleEmbedding</span><span class="p">,</span> <span class="n">StronglyEntanglingLayers</span>
<span class="kn">from</span> <span class="nn">pennylane.operation</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The second step is to define a data set. Since the performance of the models is not the focus of this demo, we can just use the first two classes of the famous <a class="reference external" href="https://en.wikipedia.org/wiki/Iris_flower_data_set">Iris data set</a>. Dating back to as far as 1936, this toy data set consists of 100 samples of four features each, and gives rise to a very simple classification problem.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># pick inputs and labels from the first two classes only,</span>
<span class="c1"># corresponding to the first 100 samples</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>

<span class="c1"># scaling the inputs is important since the embedding we use is periodic</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># scaling the labels to -1, 1 is important for the SVM and the</span>
<span class="c1"># definition of a hinge loss</span>
<span class="n">y_scaled</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y_scaled</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We use the <a class="reference external" href="https://pennylane.readthedocs.io/en/stable/code/api/pennylane.templates.embeddings.AngleEmbedding.html">angle-embedding template</a> which needs as many qubits as there are features:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_qubits</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">n_qubits</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4
</pre></div>
</div>
</div>
</div>
<p>To implement the kernel we could prepare the two states <span class="math notranslate nohighlight">\(| \phi(x) \rangle\)</span>, <span class="math notranslate nohighlight">\(| \phi(x') \rangle\)</span> on different sets of qubits with angle-embedding routines <span class="math notranslate nohighlight">\(S(x), S(x')\)</span>, and measure their overlap
with a small routine called a <a class="reference external" href="https://en.wikipedia.org/wiki/Swap_test">SWAP test</a>.</p>
<p>However, we need only half the number of qubits if we prepare <span class="math notranslate nohighlight">\(| \phi(x)\rangle\)</span> and then apply the inverse embedding with <span class="math notranslate nohighlight">\(x'\)</span> on the same qubits. We then measure the projector onto the initial state
<span class="math notranslate nohighlight">\(|0..0\rangle \langle 0..0|\)</span>.</p>
<p><img alt="" src="../../../../_images/kernel_circuit.png" />{.align-center}</p>
<p>To verify that this gives us the kernel:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\begin{align*}
    \langle 0..0 |S(x') S(x)^{\dagger} \mathcal{M} S(x')^{\dagger} S(x)  | 0..0\rangle &amp;= \langle 0..0 |S(x') S(x)^{\dagger} |0..0\rangle \langle 0..0| S(x')^{\dagger} S(x)  | 0..0\rangle  \\
    &amp;= |\langle 0..0| S(x')^{\dagger} S(x)  | 0..0\rangle |^2\\
    &amp;= | \langle \phi(x') | \phi(x)\rangle|^2 \\
    &amp;= \kappa(x, x').
\end{align*}
\end{aligned}\end{split}\]</div>
<p>Note that a projector <span class="math notranslate nohighlight">\(|0..0 \rangle \langle 0..0|\)</span> can be constructed using the <code class="docutils literal notranslate"><span class="pre">qml.Hermitian</span></code> observable in PennyLane.</p>
<p>Altogether, we use the following quantum node as a <em>quantum kernel evaluator</em>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dev_kernel</span> <span class="o">=</span> <span class="n">qml</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;default.qubit&quot;</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="n">n_qubits</span><span class="p">)</span>

<span class="n">projector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="o">**</span><span class="n">n_qubits</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="n">n_qubits</span><span class="p">))</span>
<span class="n">projector</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="nd">@qml</span><span class="o">.</span><span class="n">qnode</span><span class="p">(</span><span class="n">dev_kernel</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The quantum kernel.&quot;&quot;&quot;</span>
    <span class="n">AngleEmbedding</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">n_qubits</span><span class="p">))</span>
    <span class="n">qml</span><span class="o">.</span><span class="n">adjoint</span><span class="p">(</span><span class="n">AngleEmbedding</span><span class="p">)(</span><span class="n">x2</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">n_qubits</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">qml</span><span class="o">.</span><span class="n">expval</span><span class="p">(</span><span class="n">qml</span><span class="o">.</span><span class="n">Hermitian</span><span class="p">(</span><span class="n">projector</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">n_qubits</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<p>A good sanity check is whether evaluating the kernel of a data point and itself returns 1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kernel</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1., requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>The way an SVM with a custom kernel is implemented in scikit-learn requires us to pass a function that computes a matrix of kernel evaluations for samples in two different datasets A, B. If A=B, this is the <a class="reference external" href="https://en.wikipedia.org/wiki/Gramian_matrix">Gram matrix</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">kernel_matrix</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the matrix whose entries are the kernel</span>
<span class="sd">       evaluated on pairwise data from sets A and B.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">kernel</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">B</span><span class="p">]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">A</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Training the SVM optimizes internal parameters that basically weigh kernel functions. It is a breeze in scikit-learn, which is designed as a high-level machine learning library:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svm</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel_matrix</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s compute the accuracy on the test set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0
</pre></div>
</div>
</div>
</div>
<p>The SVM predicted all test points correctly. How many times was the quantum device evaluated?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dev_kernel</span><span class="o">.</span><span class="n">num_executions</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7501
</pre></div>
</div>
</div>
</div>
<p>This number can be derived as follows: For <span class="math notranslate nohighlight">\(M\)</span> training samples, the SVM must construct the <span class="math notranslate nohighlight">\(M \times M\)</span> dimensional kernel gram matrix for training. To classify <span class="math notranslate nohighlight">\(M_{\rm pred}\)</span> new samples, the SVM needs to evaluate the kernel at most <span class="math notranslate nohighlight">\(M_{\rm pred}M\)</span> times to get the pairwise distances between training vectors and test samples.</p>
<p>Note</p>
<p>Depending on the implementation of the SVM, only <span class="math notranslate nohighlight">\(S \leq M_{\rm pred}\)</span> <em>support vectors</em> are needed.</p>
<p>Let us formulate this as a function, which can be used at the end of the demo to construct the scaling plot shown in the introduction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">circuit_evals_kernel</span><span class="p">(</span><span class="n">n_data</span><span class="p">,</span> <span class="n">split</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute how many circuit evaluations one needs for kernel-based</span>
<span class="sd">       training and prediction.&quot;&quot;&quot;</span>

    <span class="n">M</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">split</span> <span class="o">*</span> <span class="n">n_data</span><span class="p">))</span>
    <span class="n">Mpred</span> <span class="o">=</span> <span class="n">n_data</span> <span class="o">-</span> <span class="n">M</span>

    <span class="n">n_training</span> <span class="o">=</span> <span class="n">M</span> <span class="o">*</span> <span class="n">M</span>
    <span class="n">n_prediction</span> <span class="o">=</span> <span class="n">M</span> <span class="o">*</span> <span class="n">Mpred</span>

    <span class="k">return</span> <span class="n">n_training</span> <span class="o">+</span> <span class="n">n_prediction</span>
</pre></div>
</div>
</div>
</div>
<p>With <span class="math notranslate nohighlight">\(M = 75\)</span> and <span class="math notranslate nohighlight">\(M_{\rm pred} = 25\)</span>, the number of kernel evaluations can therefore be estimated as:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">circuit_evals_kernel</span><span class="p">(</span><span class="n">n_data</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">split</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7500
</pre></div>
</div>
</div>
</div>
<p>The single additional evaluation can be attributed to evaluating the kernel once above as a sanity check.</p>
</section>
<section id="a-similar-example-using-variational-training">
<h2>A similar example using variational training<a class="headerlink" href="#a-similar-example-using-variational-training" title="Permalink to this heading">¶</a></h2>
<p>Using the variational principle of training, we can propose an <em>ansatz</em> for the variational circuit and train it directly. By increasing the number of layers of the ansatz, its expressivity increases. Depending on
the ansatz, we may only search through a subspace of all measurements for the best candidate.</p>
<p>Remember from above, the variational training does not optimize <em>exactly</em> the same cost as the SVM, but we try to match them as closely as possible. For this we use a bias term in the quantum model, and train on the hinge loss.</p>
<p>We also explicitly use the <a class="reference external" href="https://pennylane.ai/qml/glossary/parameter_shift.html">parameter-shift</a> differentiation method in the quantum node, since this is a method which works on hardware as well. While <code class="docutils literal notranslate"><span class="pre">diff_method='backprop'</span></code> or <code class="docutils literal notranslate"><span class="pre">diff_method='adjoint'</span></code> would reduce the number of circuit evaluations significantly, they are based on tricks that are only suitable for simulators, and can therefore not scale to more than a few dozen qubits.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dev_var</span> <span class="o">=</span> <span class="n">qml</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;default.qubit&quot;</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="n">n_qubits</span><span class="p">)</span>

<span class="nd">@qml</span><span class="o">.</span><span class="n">qnode</span><span class="p">(</span><span class="n">dev_var</span><span class="p">,</span> <span class="n">interface</span><span class="o">=</span><span class="s2">&quot;torch&quot;</span><span class="p">,</span> <span class="n">diff_method</span><span class="o">=</span><span class="s2">&quot;parameter-shift&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">quantum_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A variational quantum model.&quot;&quot;&quot;</span>

    <span class="c1"># embedding</span>
    <span class="n">AngleEmbedding</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">n_qubits</span><span class="p">))</span>

    <span class="c1"># trainable measurement</span>
    <span class="n">StronglyEntanglingLayers</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">n_qubits</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">qml</span><span class="o">.</span><span class="n">expval</span><span class="p">(</span><span class="n">qml</span><span class="o">.</span><span class="n">PauliZ</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">quantum_model_plus_bias</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adding a bias.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">quantum_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>

<span class="k">def</span> <span class="nf">hinge_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implements the hinge loss.&quot;&quot;&quot;</span>
    <span class="n">all_ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
    <span class="n">hinge_loss</span> <span class="o">=</span> <span class="n">all_ones</span> <span class="o">-</span> <span class="n">predictions</span> <span class="o">*</span> <span class="n">targets</span>
    <span class="c1"># trick: since the max(0,x) function is not differentiable,</span>
    <span class="c1"># use the mathematically equivalent relu instead</span>
    <span class="n">hinge_loss</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">hinge_loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hinge_loss</span>
</pre></div>
</div>
</div>
</div>
<p>We now summarize the usual training and prediction steps into two functions similar to scikit-learn’s <code class="docutils literal notranslate"><span class="pre">fit()</span></code> and <code class="docutils literal notranslate"><span class="pre">predict()</span></code>. While it feels cumbersome compared to the one-liner used to train the kernel method, PennyLane—like other differentiable programming libraries—provides a lot more control over the particulars of training.</p>
<p>In our case, most of the work is to convert between numpy and torch, which we need for the differentiable <code class="docutils literal notranslate"><span class="pre">relu</span></code> function used in the hinge loss.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">quantum_model_train</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Train the quantum model defined above.&quot;&quot;&quot;</span>

    <span class="n">params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">n_qubits</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">params_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">bias_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

    <span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">params_torch</span><span class="p">,</span> <span class="n">bias_torch</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="n">loss_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>

        <span class="n">batch_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">batch_ids</span><span class="p">]</span>
        <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">batch_ids</span><span class="p">]</span>

        <span class="n">X_batch_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_batch</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">y_batch_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_batch</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">closure</span><span class="p">():</span>
            <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
                <span class="p">[</span><span class="n">quantum_model_plus_bias</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params_torch</span><span class="p">,</span> <span class="n">bias_torch</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_batch_torch</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">hinge_loss</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">y_batch_torch</span><span class="p">))</span>

            <span class="c1"># bookkeeping</span>
            <span class="n">current_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_loss</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;step&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="s2">&quot;, loss&quot;</span><span class="p">,</span> <span class="n">current_loss</span><span class="p">)</span>

            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">loss</span>

        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">params_torch</span><span class="p">,</span> <span class="n">bias_torch</span><span class="p">,</span> <span class="n">loss_history</span>


<span class="k">def</span> <span class="nf">quantum_model_predict</span><span class="p">(</span><span class="n">X_pred</span><span class="p">,</span> <span class="n">trained_params</span><span class="p">,</span> <span class="n">trained_bias</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict using the quantum model defined above.&quot;&quot;&quot;</span>

    <span class="n">p</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_pred</span><span class="p">:</span>

        <span class="n">x_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">pred_torch</span> <span class="o">=</span> <span class="n">quantum_model_plus_bias</span><span class="p">(</span><span class="n">x_torch</span><span class="p">,</span> <span class="n">trained_params</span><span class="p">,</span> <span class="n">trained_bias</span><span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">pred_torch</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">pred</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="n">p</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s train the variational model and see how well we are doing on the test set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">trained_params</span><span class="p">,</span> <span class="n">trained_bias</span><span class="p">,</span> <span class="n">loss_history</span> <span class="o">=</span> <span class="n">quantum_model_train</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

<span class="n">pred_test</span> <span class="o">=</span> <span class="n">quantum_model_predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">trained_params</span><span class="p">,</span> <span class="n">trained_bias</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;accuracy on test set:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">pred_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_history</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;cost&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>step 0 , loss 1.2128428849025235
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>step 10 , loss 0.8582750956106431
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>step 20 , loss 0.43849890579633233
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>step 30 , loss 0.6458829274590642
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>step 40 , loss 0.5540116701446128
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>step 50 , loss 0.4132239145818266
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>step 60 , loss 0.52094330038141
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>step 70 , loss 0.4694193423160377
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>step 80 , loss 0.48581457440211384
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>step 90 , loss 0.41962346215340246
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>accuracy on test set: 0.96
</pre></div>
</div>
<img alt="../../../../_images/f0f145c925c368c06803fd64ef4cf4c031a1908f4b83f519c740660f52716a51.png" src="../../../../_images/f0f145c925c368c06803fd64ef4cf4c031a1908f4b83f519c740660f52716a51.png" />
</div>
</div>
<p>The variational circuit has a slightly lower accuracy than the SVM—but this depends very much on the training settings we used. Different random parameter initializations, more layers, or more steps may indeed get perfect test accuracy.</p>
<p>How often was the device executed?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dev_var</span><span class="o">.</span><span class="n">num_executions</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>74025
</pre></div>
</div>
</div>
</div>
<p>That is a lot more than the kernel method took!</p>
<p>Let’s try to understand this value. In each optimization step, the variational circuit needs to compute the partial derivative of all trainable parameters for each sample in a batch. Using parameter-shift
rules, we require roughly two circuit evaluations per partial derivative. Prediction uses only one circuit evaluation per sample.</p>
<p>We can formulate this as another function that will be used in the scaling plot below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">circuit_evals_variational</span><span class="p">(</span><span class="n">n_data</span><span class="p">,</span> <span class="n">n_params</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">shift_terms</span><span class="p">,</span> <span class="n">split</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute how many circuit evaluations are needed for</span>
<span class="sd">       variational training and prediction.&quot;&quot;&quot;</span>

    <span class="n">M</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">split</span> <span class="o">*</span> <span class="n">n_data</span><span class="p">))</span>
    <span class="n">Mpred</span> <span class="o">=</span> <span class="n">n_data</span> <span class="o">-</span> <span class="n">M</span>

    <span class="n">n_training</span> <span class="o">=</span> <span class="n">n_params</span> <span class="o">*</span> <span class="n">n_steps</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">shift_terms</span>
    <span class="n">n_prediction</span> <span class="o">=</span> <span class="n">Mpred</span>

    <span class="k">return</span> <span class="n">n_training</span> <span class="o">+</span> <span class="n">n_prediction</span>
</pre></div>
</div>
</div>
</div>
<p>This estimates the circuit evaluations in variational training as:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">circuit_evals_variational</span><span class="p">(</span>
    <span class="n">n_data</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>
    <span class="n">n_params</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">trained_params</span><span class="o">.</span><span class="n">flatten</span><span class="p">()),</span>
    <span class="n">n_steps</span><span class="o">=</span><span class="n">steps</span><span class="p">,</span>
    <span class="n">shift_terms</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>96025
</pre></div>
</div>
</div>
</div>
<p>The estimate is a bit higher because it does not account for some optimizations that PennyLane performs under the hood.</p>
<p>It is important to note that while they are trained in a similar manner, the number of variational circuit evaluations differs from the number of neural network model evaluations in classical machine learning, which would be given by:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model_evals_nn</span><span class="p">(</span><span class="n">n_data</span><span class="p">,</span> <span class="n">n_params</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">split</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute how many model evaluations are needed for neural</span>
<span class="sd">       network training and prediction.&quot;&quot;&quot;</span>

    <span class="n">M</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">split</span> <span class="o">*</span> <span class="n">n_data</span><span class="p">))</span>
    <span class="n">Mpred</span> <span class="o">=</span> <span class="n">n_data</span> <span class="o">-</span> <span class="n">M</span>

    <span class="n">n_training</span> <span class="o">=</span> <span class="n">n_steps</span> <span class="o">*</span> <span class="n">batch_size</span>
    <span class="n">n_prediction</span> <span class="o">=</span> <span class="n">Mpred</span>

    <span class="k">return</span> <span class="n">n_training</span> <span class="o">+</span> <span class="n">n_prediction</span>
</pre></div>
</div>
</div>
</div>
<p>In each step of neural network training, and due to the clever implementations of automatic differentiation, the backpropagation algorithm can compute a gradient for all parameters in (more-or-less) a single run. For all we know at this stage, the no-cloning principle prevents variational circuits from using these tricks, which leads to <code class="docutils literal notranslate"><span class="pre">n_training</span></code> in <code class="docutils literal notranslate"><span class="pre">circuit_evals_variational</span></code> depending on the number of parameters, but not in <code class="docutils literal notranslate"><span class="pre">model_evals_nn</span></code>.</p>
<p>For the same example as used here, a neural network would therefore have far fewer model evaluations than both variational and kernel-based training:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_evals_nn</span><span class="p">(</span>
    <span class="n">n_data</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>
    <span class="n">n_params</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">trained_params</span><span class="o">.</span><span class="n">flatten</span><span class="p">()),</span>
    <span class="n">n_steps</span><span class="o">=</span><span class="n">steps</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2025
</pre></div>
</div>
</div>
</div>
</section>
<section id="which-method-scales-best">
<h2>Which method scales best?<a class="headerlink" href="#which-method-scales-best" title="Permalink to this heading">¶</a></h2>
<p>The answer to this question depends on how the variational model is set up, and we need to make a few assumptions:</p>
<ol class="arabic">
<li><p>Even if we use single-batch stochastic gradient descent, in which
every training step uses exactly one training sample, we would want
to see every training sample at least once on average. Therefore,
the number of steps should scale at least linearly with the number
of training data samples.</p></li>
<li><p>Modern neural networks often have many more parameters than training
samples. But we do not know yet whether variational circuits really
need that many parameters as well. We will therefore use two cases
for comparison:</p>
<p>2a) the number of parameters grows linearly with the training data,
or <code class="docutils literal notranslate"><span class="pre">n_params</span> <span class="pre">=</span> <span class="pre">M</span></code>,</p>
<p>2b) the number of parameters saturates at some point, which we model
by setting <code class="docutils literal notranslate"><span class="pre">n_params</span> <span class="pre">=</span> <span class="pre">sqrt(M)</span></code>.</p>
</li>
</ol>
<p>Note that compared to the example above with 75 training samples and 24 parameters, a) overestimates the number of evaluations, while b) underestimates it.</p>
<p>This is how the three methods compare:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">variational_training1</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">variational_training2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">kernelbased_training</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">nn_training</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">x_axis</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="k">for</span> <span class="n">M</span> <span class="ow">in</span> <span class="n">x_axis</span><span class="p">:</span>

    <span class="n">var1</span> <span class="o">=</span> <span class="n">circuit_evals_variational</span><span class="p">(</span>
        <span class="n">n_data</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">n_params</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="n">M</span><span class="p">,</span>  <span class="n">shift_terms</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">variational_training1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">var1</span><span class="p">)</span>

    <span class="n">var2</span> <span class="o">=</span> <span class="n">circuit_evals_variational</span><span class="p">(</span>
        <span class="n">n_data</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">n_params</span><span class="o">=</span><span class="nb">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">M</span><span class="p">)),</span> <span class="n">n_steps</span><span class="o">=</span><span class="n">M</span><span class="p">,</span>
        <span class="n">shift_terms</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">variational_training2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">var2</span><span class="p">)</span>

    <span class="n">kernel</span> <span class="o">=</span> <span class="n">circuit_evals_kernel</span><span class="p">(</span><span class="n">n_data</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
    <span class="n">kernelbased_training</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>

    <span class="n">nn</span> <span class="o">=</span> <span class="n">model_evals_nn</span><span class="p">(</span>
        <span class="n">n_data</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">n_params</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">nn_training</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">nn_training</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;neural net&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">variational_training1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;var. circuit (linear param scaling)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">variational_training2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;var. circuit (srqt param scaling)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">kernelbased_training</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;(quantum) kernel&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;size of data set&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;number of evaluations&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/f80db12296cf99179331452e50061cc4e401fa726bd1a461cad7f8357e681a44.png" src="../../../../_images/f80db12296cf99179331452e50061cc4e401fa726bd1a461cad7f8357e681a44.png" />
</div>
</div>
<p>This is the plot we saw at the beginning. With current hardware-compatible training methods, whether kernel-based training requires more or fewer quantum circuit evaluations than variational training depends on how many parameters the latter needs. If variational circuits turn out to be as parameter-hungry as neural networks, kernel-based training will outperform them for common machine learning tasks. However, if variational learning only turns out to require few parameters (or if more efficient training methods are found), variational circuits could in principle match the linear scaling of neural networks trained with backpropagation.</p>
<p>The practical take-away from this demo is that unless your variational circuit has significantly fewer parameters than training data, kernel methods could be a much faster alternative!</p>
<p>Finally, it is important to note that fault-tolerant quantum computers may change the picture for both quantum and classical machine learning. As mentioned in <a class="reference external" href="https://arxiv.org/abs/2101.11020">Schuld (2021)</a>, early results from the quantum machine learning literature show that larger quantum computers will most likely enable us to reduce the quadratic scaling of kernel methods to linear scaling, which may make classical as well as quantum kernel methods a strong alternative to neural networks for big data processing one day.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="send-it-after-class">
<h1>Send it after class<a class="headerlink" href="#send-it-after-class" title="Permalink to this heading">¶</a></h1>
<p>Create a dataset which is nonlinear and then apply the kernel method you have just seen to classify the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">417</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/e1a1b7462f8e370d102fb5f11a7643e8143f690d4dace3c20601a3fbdf20a906.png" src="../../../../_images/e1a1b7462f8e370d102fb5f11a7643e8143f690d4dace3c20601a3fbdf20a906.png" />
</div>
</div>
</section>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../../../../_sources/courses/PHYS710/hands-on/hands-on-7/hands-on-7-book.ipynb.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright (CC BY 3.0) https://creativecommons.org/ .<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 5.3.0.<br/>
    </p>
  </div>
</footer>
  </body>
</html>